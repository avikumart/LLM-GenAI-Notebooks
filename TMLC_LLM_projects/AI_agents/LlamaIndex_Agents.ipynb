{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/TMLC_LLM_projects/AI_agents/LlamaIndex_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing libraries"
      ],
      "metadata": {
        "id": "neaaA1qcSzSb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G58UZsatESIa",
        "outputId": "2abe8cea-c913-4d98-9dea-27a2982b4b7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.6/250.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install llama-index duckduckgo-search llama-index-llms-openai openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraires, llm and define custom tools"
      ],
      "metadata": {
        "id": "JIl2p62ES31x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "OpenAI_API = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = OpenAI_API"
      ],
      "metadata": {
        "id": "kBN88WlZJ1qA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.openai import OpenAI  # Import the OpenAI class to interact with OpenAI's language models\n",
        "from llama_index.core.llms import ChatMessage  # ChatMessage class to structure and handle messages for chat-based interactions\n",
        "from llama_index.core.tools import BaseTool, FunctionTool  # BaseTool and FunctionTool for building and managing tools within LlamaIndex"
      ],
      "metadata": {
        "id": "TfONuj5nJ7aU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two integers and return the result.\"\"\"\n",
        "    return a * b  # Perform multiplication and return the result\n",
        "\n",
        "# Create a FunctionTool instance for the 'multiply' function, enabling it to be used as a tool\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and return the result.\"\"\"\n",
        "    return a + b  # Perform addition and return the result\n",
        "\n",
        "# Create a FunctionTool instance for the 'add' function, enabling it to be used as a tool\n",
        "add_tool = FunctionTool.from_defaults(fn=add)"
      ],
      "metadata": {
        "id": "lsz4FFRvKD4A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(model=\"gpt-4o-mini\")  # Initialize an OpenAI language model instance with the specified model (\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "u0kLuytVKG9p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaIndex handles memory by itself"
      ],
      "metadata": {
        "id": "sw0y50ouTtqC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling Agent"
      ],
      "metadata": {
        "id": "M6wZr9WjMumw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import FunctionCallingAgentWorker  # Import a worker class for managing tool-based function calls\n",
        "from llama_index.core.agent import AgentRunner  # Import the AgentRunner class for running and managing agents\n",
        "\n",
        "# Initialize a FunctionCallingAgentWorker with the tools and LLM\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    [multiply_tool, add_tool],  # List of tools (functions wrapped as tools) available to the agent\n",
        "    llm=llm,  # Specify the language model (e.g., OpenAI's GPT-4o-mini) for the agent\n",
        "    verbose=True,  # Enable detailed logging for debugging or monitoring agent activities\n",
        "    allow_parallel_tool_calls=False,  # Restrict the agent to call one tool at a time, preventing parallel executions\n",
        ")\n",
        "\n",
        "# Create an AgentRunner to execute tasks using the agent worker\n",
        "agent = AgentRunner(agent_worker)"
      ],
      "metadata": {
        "id": "ceqwC5Z3KNhg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\"What is (121 + 2) * 5?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbxhbd_OKOfD",
        "outputId": "f9bd8f62-5964-4930-fa53-0df281e60661"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: What is (121 + 2) * 5?\n",
            "=== Calling Function ===\n",
            "Calling function: add with args: {\"a\": 121, \"b\": 2}\n",
            "=== Function Output ===\n",
            "123\n",
            "=== Calling Function ===\n",
            "Calling function: multiply with args: {\"a\": 123, \"b\": 5}\n",
            "=== Function Output ===\n",
            "615\n",
            "=== LLM Response ===\n",
            "The result of (121 + 2) * 5 is 615.\n",
            "The result of (121 + 2) * 5 is 615.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.sources) # output of the tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H67WtA8qKPy_",
        "outputId": "9cf9363e-f936-450b-a5e5-54b43ce85da6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ToolOutput(content='123', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 121, 'b': 2}}, raw_output=123, is_error=False), ToolOutput(content='615', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 123, 'b': 5}}, raw_output=615, is_error=False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## React Agent"
      ],
      "metadata": {
        "id": "uFgeK8e0Mxq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import ReActAgent  # Import the ReActAgent class, designed for reasoning and acting with tools\n",
        "\n",
        "# Initialize a ReActAgent with the tools and LLM\n",
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool],  # Provide the list of tools (e.g., multiply and add functions wrapped as tools)\n",
        "    llm=llm,  # Specify the language model to use for reasoning and generating responses\n",
        "    verbose=True,  # Enable detailed logging for debugging or understanding the agent's reasoning process\n",
        ")\n",
        "\n",
        "# Use the agent to process a chat query involving arithmetic calculations\n",
        "response = agent.chat(\"What is (121 * 3) + (5 * 8)?\")\n",
        "\n",
        "# Print the agent's response as a string\n",
        "print(str(response))  # Convert the response to a string for display"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyb4NfPnKkiM",
        "outputId": "768b6025-40ec-4449-ee36-e17b1d8cbff4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step aa544d40-04d4-479c-a7a9-6cb4d6997dc3. Step input: What is (121 * 3) + (5 * 8)?\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use the multiply and add tools to help me answer the question.\n",
            "Action: multiply\n",
            "Action Input: {'a': 121, 'b': 3}\n",
            "\u001b[0m\u001b[1;3;34mObservation: 363\n",
            "\u001b[0m> Running step 6de2961b-4e63-40a9-a7e3-6fea18cc268e. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
            "Answer: Action: multiply\n",
            "Action Input: {\"a\": 5, \"b\": 8}\n",
            "\u001b[0mAction: multiply\n",
            "Action Input: {\"a\": 5, \"b\": 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function Calling Agent vs ReAct Agent\n",
        "\n",
        "The decision between ReACT agents and function-calling agents depends on the nature of the task. For tasks requiring complex reasoning and actions that cannot be easily represented as functions, ReACT agents are often more suitable. On the other hand, if the task involves interacting with specific APIs or performing clearly defined actions, function-calling agents may be a better fit. Both approaches provide powerful tools to enhance LLM capabilities and enable innovative human-machine collaboration."
      ],
      "metadata": {
        "id": "U7bR__tSTMNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structured Outputs\n",
        "\n",
        "Special function for OpenAI llms"
      ],
      "metadata": {
        "id": "Y1Q16pPiM6Tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.prompts import PromptTemplate  # Import PromptTemplate for creating structured prompts\n",
        "from pydantic import BaseModel  # Import BaseModel from Pydantic for data modeling\n",
        "from typing import List  # Import List for type hinting lists of items\n",
        "\n",
        "# Define a Pydantic model for ticket pricing details\n",
        "class TicketInfo(BaseModel):\n",
        "    \"\"\"Information about ticket pricing.\"\"\"\n",
        "    type: str  # Type of ticket (e.g., adult, child)\n",
        "    price: float  # Price of the ticket\n",
        "\n",
        "# Define a Pydantic model for opening hours of tourist attractions\n",
        "class OpeningHours(BaseModel):\n",
        "    \"\"\"Opening hours for each day of the week.\"\"\"\n",
        "    day: str  # Day of the week (e.g., Monday)\n",
        "    open_time: str  # Opening time (e.g., \"9:00 AM\")\n",
        "    close_time: str  # Closing time (e.g., \"5:00 PM\")\n",
        "\n",
        "# Define a Pydantic model for a tourist attraction\n",
        "class TouristAttraction(BaseModel):\n",
        "    \"\"\"A tourist attraction in a city.\"\"\"\n",
        "    name: str  # Name of the tourist attraction (e.g., Eiffel Tower)\n",
        "    city: str  # City where the attraction is located (e.g., Paris)\n",
        "    description: str  # Description of the attraction\n",
        "    ticket_info: List[TicketInfo]  # List of TicketInfo objects related to the attraction\n",
        "    opening_hours: List[OpeningHours]  # List of OpeningHours objects for the attraction\n",
        "\n",
        "# Initialize the OpenAI LLM with the GPT-3.5 Turbo model\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Define the prompt template to generate details for a tourist attraction in a specified city\n",
        "prompt_tmpl = PromptTemplate(\n",
        "    \"Generate details of a famous tourist attraction in {city_name}.\"  # Template that will be formatted with city name\n",
        ")\n",
        "\n",
        "# Use the LLM to complete the prompt for a specific city (Paris) and structure the result as a TouristAttraction object\n",
        "response = (\n",
        "    llm.as_structured_llm(TouristAttraction)  # Use the LLM with structured output for TouristAttraction\n",
        "    .complete(prompt_tmpl.format(city_name=\"Paris\"))  # Format the prompt with the city name and generate the response\n",
        "    .raw  # Get the raw response as a structured object\n",
        ")\n",
        "\n",
        "# Print the structured object containing the tourist attraction details\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4dCwE7CLA2A",
        "outputId": "5a37fb4e-94b9-450c-a797-c61bcd8fc456"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Eiffel Tower' city='Paris' description='The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is one of the most iconic landmarks in the world and a symbol of France.' ticket_info=[TicketInfo(type='Standard Ticket', price=25.0), TicketInfo(type='Top Floor Ticket', price=50.0)] opening_hours=[OpeningHours(day='Monday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Tuesday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Wednesday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Thursday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Friday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Saturday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Sunday', open_time='9:30 AM', close_time='11:45 PM')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "oaTwjj9lZTnK",
        "outputId": "f9184ff5-8e22-4364-abe6-20030ae20ace"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "TouristAttraction(name='Eiffel Tower', city='Paris', description='The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is one of the most iconic landmarks in the world and a symbol of France.', ticket_info=[TicketInfo(type='Standard', price=25.0), TicketInfo(type='Top Floor', price=50.0)], opening_hours=[OpeningHours(day='Monday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Tuesday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Wednesday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Thursday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Friday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Saturday', open_time='9:30 AM', close_time='11:45 PM'), OpeningHours(day='Sunday', open_time='9:30 AM', close_time='11:45 PM')])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent with Web Search\n",
        "\n",
        "The code is implemented in 3 different ways:\n",
        "1. Direct LLM call Agent\n",
        "2. Function calling Agent\n",
        "3. Agent Runner Agent"
      ],
      "metadata": {
        "id": "XLKX5sDvNBWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS"
      ],
      "metadata": {
        "id": "YH7PnoVwLE82"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query: str) -> str:\n",
        "\n",
        "    req = DDGS()  # Initialize the DDGS (DuckDuckGo Search) object to perform the search\n",
        "    response = req.text(query, max_results=4)  # Perform the search and limit results to 4\n",
        "\n",
        "    context = \"\"  # Initialize an empty string to store the combined result texts\n",
        "    for result in response:  # Iterate over the search results\n",
        "        context += result['body']  # Concatenate the body of each result into the context\n",
        "\n",
        "    return context  # Return the concatenated text from the search results\n",
        "\n",
        "# Create a FunctionTool instance for the search function, making it available as a tool\n",
        "search_tool = FunctionTool.from_defaults(fn=search)"
      ],
      "metadata": {
        "id": "ucwxC3Q1L67P"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings  # Import the Settings class from the llama_index.core module\n",
        "llm = OpenAI(model=\"gpt-4o-mini\")\n",
        "Settings.llm = llm # Set the LLM (Language Model) to be used globally in the LlamaIndex framework"
      ],
      "metadata": {
        "id": "lyL_7R9rL9Dk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without tool"
      ],
      "metadata": {
        "id": "oL9mzyn_UpOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.complete(\"Who won Border-Gavaskar Trophy (BGT) 2024-2025?\").text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLzyw-0tMCKA",
        "outputId": "6513571a-6886-431a-9b19-d336fa34707d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I don't have information on events that occurred after October 2023, including the Border-Gavaskar Trophy for 2024-2025. You may want to check the latest sports news or official cricket websites for the most current information.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Direct LLM call with tool"
      ],
      "metadata": {
        "id": "ywFNTBvOUrIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Who won Border-Gavaskar Trophy (BGT) 2024-2025?\"  # Define the query string to ask about the winner of the BGT 2024-2025\n",
        "\n",
        "# Use the LLM to predict and call the search_tool to find relevant information about the query\n",
        "function_call = llm.predict_and_call(\n",
        "    [search_tool],  # List of tools (e.g., search_tool) the LLM can use to gather information\n",
        "    user_msg=query,  # The user's query message\n",
        "    allow_parallel_tool_calls=True,  # Allow parallel execution of tool calls (multiple tools can be called at once)\n",
        ")\n",
        "\n",
        "# Print the response from the LLM after it has processed the query and used the search tool\n",
        "print(function_call.response)  # Display the result (likely the winner of the BGT 2024-2025)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-RxGmbcMFMr",
        "outputId": "e5f65351-fb9f-4f76-9392-a9991c13024a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Australia won the Border Gavaskar Trophy back after clinching India vs Australia five-match Test series 3-1 at Sydney Cricket Ground on January 5, 2025. A target of 162 could have been trickier had new Test captain Jasprit Bumrah been in a position to bowl despite painful back spasms but once Virat ...Discover the complete list of Border-Gavaskar Trophy winners till 2024. Get insights into the epic India vs Australia Test series, trophy history, and memorable wins.The Border-Gavaskar Trophy 2024-2025 has already delivered thrilling cricketing action, with all five Tests now completed. Played across Australia's iconic venues, this five-Test series challenged players with its demanding format, testing their skill, strategy, and endurance.The Border-Gavaskar Trophy (colloquially known as BGT) [3] is an International Test cricket trophy played between India and Australia. The series is named after distinguished former captains, Australia's Allan Border and India's Sunil Gavaskar. It is played via Test series scheduled using International Cricket Council 's Future Tours Programme. The winner of a Test series wins the trophy. If ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function Calling"
      ],
      "metadata": {
        "id": "yNrqCG06UuMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import FunctionCallingAgent, FunctionCallingAgentWorker  # Import agent classes for managing tool calls\n",
        "\n",
        "# Initialize the FunctionCallingAgent with the specified tools and LLM\n",
        "agent = FunctionCallingAgent.from_tools(\n",
        "    [search_tool],  # Provide the list of tools (e.g., search_tool) that the agent can use\n",
        "    llm=llm,  # Specify the LLM (e.g., OpenAI model) to be used for reasoning and generating responses\n",
        "    verbose=True,  # Enable verbose mode to log detailed agent actions and reasoning steps\n",
        "    allow_parallel_tool_calls=True,  # Allow the agent to call multiple tools in parallel if needed\n",
        ")\n",
        "\n",
        "# Define the query to be processed by the agent\n",
        "query = \"Who won Border-Gavaskar Trophy (BGT) 2024-2025?\"\n",
        "\n",
        "# Use the agent to process the query and generate a response\n",
        "response = agent.chat(query)  # The agent processes the query, utilizes tools, and returns the response\n",
        "\n",
        "# Print the response generated by the agent\n",
        "print(response)  # Display the result, which should contain the answer to the query (e.g., BGT winner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQWb5e-dMYSd",
        "outputId": "d8692198-2a9a-4f37-8c43-6618ce4667c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Running step 05d452e4-98a0-4104-98f9-1d88218a710e. Step input: Who won Border-Gavaskar Trophy (BGT) 2024-2025?\n",
            "Added user message to memory: Who won Border-Gavaskar Trophy (BGT) 2024-2025?\n",
            "=== Calling Function ===\n",
            "Calling function: search with args: {\"query\": \"Border-Gavaskar Trophy 2024-2025 winner\"}\n",
            "=== Function Output ===\n",
            "Australia won the Border Gavaskar Trophy back after clinching India vs Australia five-match Test series 3-1 at Sydney Cricket Ground on January 5, 2025. A target of 162 could have been trickier had new Test captain Jasprit Bumrah been in a position to bowl despite painful back spasms but once Virat ...The winner of a Test series wins the trophy. If a series is drawn, the country holding the trophy retains it. ... As of January 2025, Australia hold the trophy after they defeated India 3-1 in the 2024-25 series. ... 2024-25 series The Border-Gavaskar Trophy is a prestigious Test cricket series contested between India and Australia, named ...Discover the complete list of Border-Gavaskar Trophy winners till 2024. Get insights into the epic India vs Australia Test series, trophy history, and memorable wins.Border-Gavaskar Trophy 2024/2025 - Schedule, Venues & Results The 2024-2025 Border-Gavaskar Trophy, hosted by Australia, has now concluded with all five Test matches. The series, played across various venues, provided both teams with ample opportunities to showcase their skills and compete for supremacy in this prestigious contest.\n",
            "> Running step e071fe86-f0e3-4582-9357-1e79ace9acbe. Step input: None\n",
            "=== LLM Response ===\n",
            "Australia won the Border-Gavaskar Trophy (BGT) for the 2024-2025 series by defeating India 3-1 in a five-match Test series, which concluded on January 5, 2025, at the Sydney Cricket Ground.\n",
            "Australia won the Border-Gavaskar Trophy (BGT) for the 2024-2025 series by defeating India 3-1 in a five-match Test series, which concluded on January 5, 2025, at the Sydney Cricket Ground.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Runner over Function Calling Agent"
      ],
      "metadata": {
        "id": "xwHcU0rIUwCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import AgentRunner  # Import the AgentRunner class to manage agent workflows\n",
        "\n",
        "# Initialize the FunctionCallingAgentWorker with the specified tools and LLM\n",
        "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
        "    [search_tool],  # Provide the list of tools (e.g., search_tool) that the agent can use\n",
        "    llm=llm,  # Specify the LLM (e.g., OpenAI model) to be used for reasoning and generating responses\n",
        "    verbose=True,  # Enable verbose mode to log detailed agent actions and reasoning steps\n",
        "    allow_parallel_tool_calls=True,  # Allow the agent to call multiple tools in parallel if needed\n",
        ")\n",
        "\n",
        "# Create an AgentRunner instance using the agent worker\n",
        "agent_runner = AgentRunner(agent_worker)  # AgentRunner will manage the execution of the agent workflow\n",
        "\n",
        "# Define the query to be processed by the agent\n",
        "query = \"Who won Border-Gavaskar Trophy (BGT) 2024-2025?\"\n",
        "\n",
        "# Use the AgentRunner to process the query and generate a response\n",
        "runner_chat = agent_runner.chat(query)  # The agent runner invokes the agent and handles the response\n",
        "\n",
        "# Print the response generated by the agent\n",
        "print(runner_chat.response)  # Display the result, which should contain the answer to the query (e.g., BGT winner)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kETmtKgCMd6f",
        "outputId": "67bed0ec-559d-47d4-ed4e-237ba83164da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added user message to memory: Who won Border-Gavaskar Trophy (BGT) 2024-2025?\n",
            "=== Calling Function ===\n",
            "Calling function: search with args: {\"query\": \"Border-Gavaskar Trophy 2024-2025 winner\"}\n",
            "=== Function Output ===\n",
            "The winner of a Test series wins the trophy. If a series is drawn, the country holding the trophy retains it. ... As of January 2025, Australia hold the trophy after they defeated India 3-1 in the 2024-25 series. ... 2024-25 series The Border-Gavaskar Trophy is a prestigious Test cricket series contested between India and Australia, named ...Australia won the Border Gavaskar Trophy back after clinching India vs Australia five-match Test series 3-1 at Sydney Cricket Ground on January 5, 2025. A target of 162 could have been trickier had new Test captain Jasprit Bumrah been in a position to bowl despite painful back spasms but once Virat ...Discover the complete list of Border-Gavaskar Trophy winners till 2024. Get insights into the epic India vs Australia Test series, trophy history, and memorable wins.Border-Gavaskar Trophy 2024-25 Awards. BGT Winners: Australia; IND vs AUS 5th Test Player of the Match: Scott Boland (Australia) - 10 for 76 - 4 wickets in the first innings & 6 wickets in the second innings; BGT 2024/25 Player of the Series: Jasprit Bumrah (India) - 32 wickets in 9 innings at an average of 13.06\n",
            "=== LLM Response ===\n",
            "Australia won the Border-Gavaskar Trophy (BGT) for the 2024-2025 series, defeating India 3-1. The final match took place at the Sydney Cricket Ground on January 5, 2025. Scott Boland was named Player of the Match for his outstanding performance, while Jasprit Bumrah was awarded Player of the Series for taking 32 wickets in 9 innings.\n",
            "Australia won the Border-Gavaskar Trophy (BGT) for the 2024-2025 series, defeating India 3-1. The final match took place at the Sydney Cricket Ground on January 5, 2025. Scott Boland was named Player of the Match for his outstanding performance, while Jasprit Bumrah was awarded Player of the Series for taking 32 wickets in 9 innings.\n"
          ]
        }
      ]
    }
  ]
}