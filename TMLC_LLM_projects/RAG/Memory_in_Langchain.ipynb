{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/TMLC_LLM_projects/RAG/Memory_in_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing necessary libraries"
      ],
      "metadata": {
        "id": "8UblPEH_V5FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cohere provides free trial keys to use their LLMs. So generate one trial key from dashboard.cohere.com"
      ],
      "metadata": {
        "id": "MHAhn6gkWIbK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8HmgD6eLRrzA",
        "outputId": "b5193a78-b8f8-4f71-ccb4-95c29465df36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.2/606.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.2/250.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 1.4.20 requires pydantic>=2.7.0, but you have pydantic 1.10.8 which is incompatible.\n",
            "google-genai 0.3.0 requires pydantic<3.0.0dev,>=2.0.0, but you have pydantic 1.10.8 which is incompatible.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
            "wandb 0.19.1 requires pydantic<3,>=2.6, but you have pydantic 1.10.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain-cohere langchain pdfminer.six chromadb pydantic==1.10.8 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "langchain-cohere: Enables integration of Cohere's language models with LangChain for advanced text generation and processing workflows.\n",
        "\n",
        "langchain: Provides a modular framework for building language model-powered applications, such as chatbots, question-answering systems, and conversational agents.\n",
        "\n",
        "pdfminer.six: Facilitates text extraction from PDF files, making it useful for document analysis and preprocessing tasks.\n",
        "\n",
        "chromadb: A vector database library designed for efficient storage and retrieval of embeddings, ideal for tasks like semantic search and recommendation systems."
      ],
      "metadata": {
        "id": "dYqZZciyV12N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "FAFkZ2UdV71s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.messages import BaseMessage, AIMessage,HumanMessage\n",
        "from google.colab import userdata\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_KEY')\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from pdfminer.high_level import extract_text as extract_text_pdf_miner\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_core.runnables import RunnableParallel,RunnablePassthrough\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "RpRhM8WhR2Vc",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VectorDB setup"
      ],
      "metadata": {
        "id": "DnFFjExUWAzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where the Chroma database will persist data\n",
        "persist_directory = \"/content/chroma_db\"\n",
        "\n",
        "# Initialize Cohere embeddings with the specified model\n",
        "# \"embed-english-v3.0\" is a pre-trained English language embedding model by Cohere\n",
        "# The user_agent parameter specifies the tool or library using the Cohere API, in this case, LangChain\n",
        "embedding = CohereEmbeddings(\n",
        "    model=\"embed-english-v3.0\",\n",
        "    user_agent=\"langchain\"\n",
        ")"
      ],
      "metadata": {
        "id": "92kuclutTc9j",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are processing 2 research papers on ai agents and genai on vertex ai. You can use the PDFs"
      ],
      "metadata": {
        "id": "68hrmDRbWmD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through a list of PDF files to process\n",
        "for pdf_name in [\"/content/Newwhitepaper_Agents.pdf\", \"/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf\"]:\n",
        "    # Open each PDF file in binary mode\n",
        "    with open(pdf_name, 'rb') as f:\n",
        "        # Extract text from the PDF using the extract_text_pdf_miner function\n",
        "        text = extract_text_pdf_miner(f)\n",
        "\n",
        "        # Clean the extracted text by removing newline characters and joining into a single string\n",
        "        cleaned_text = \" \".join(text.split(\"\\n\"))\n",
        "\n",
        "        # Initialize a list to store document chunks\n",
        "        docs = []\n",
        "\n",
        "        # Create a text splitter to divide the text into manageable chunks\n",
        "        # Each chunk has a maximum size of 2048 characters with a 512-character overlap\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
        "\n",
        "        # Split the cleaned text into chunks and wrap each chunk in a Document object\n",
        "        for chunk in splitter.split_text(cleaned_text):\n",
        "            docs.append(Document(page_content=chunk, metadata={\"source\": pdf_name}))\n",
        "\n",
        "    # Create a Chroma collection from the processed documents\n",
        "    # Use the specified persist directory and embedding model for storage and retrieval\n",
        "    vector_collection_fixed_size = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        persist_directory=persist_directory,\n",
        "        embedding=embedding\n",
        "    )"
      ],
      "metadata": {
        "id": "5lGwexERTCCx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Chroma vector database\n",
        "# The persist_directory specifies the location where the database is stored\n",
        "# The embedding_function parameter provides the embedding model used for vector representation\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wmFXQin5Ulz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "298dcd5c-9871-4361-cd39-23cbb084aca0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-66dc66a8b09c>:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
            "  vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a similarity search on the vector database\n",
        "# The query \"What is YOLO?\" is used to find the most relevant documents\n",
        "# k=1 specifies that the top 1 most similar document should be retrieved\n",
        "# The method also returns relevance scores indicating how closely each document matches the query\n",
        "vectordb.similarity_search_with_relevance_scores(\"What is YOLO?\", k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykJDpWIXU8r9",
        "outputId": "195c0a57-8b96-4ddf-e50b-e3c2ea707839"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(metadata={'source': '/content/1506.02640v5.pdf'}, page_content='Detection In The Wild  Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and  YOLO is a fast, accurate object detector, making it ideal for computer vision applications. We connect YOLO to a webcam and verify that it maintains real-time performance,  \\x0cVOC 2007 AP 59.2 54.2 43.2 36.5 -  Picasso AP Best F1 0.590 53.3 0.226 10.4 0.458 37.8 0.271 17.8 0.051 1.9  People-Art AP 45 26 32  YOLO R-CNN DPM Poselets [2] D&T [4]  (a) Picasso Dataset precision-recall curves.  (b) Quantitative results on the VOC 2007, Picasso, and People-Art Datasets. The Picasso Dataset evaluates on both AP and best F1 score.  Figure 5: Generalization results on Picasso and People-Art datasets.  Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it does think one person is an airplane.  including the time to fetch images from the camera and dis- play the detections.  The resulting system is interactive and engaging. While YOLO processes images individually, when attached to a webcam it functions like a tracking system, detecting ob- jects as they move around and change in appearance. A demo of the system and the source code can be found on our project website: http://pjreddie.com/yolo/.  6. Conclusion  We introduce YOLO, a uniﬁed model for object detec- tion. Our model is simple to construct and can be trained  directly on full images. Unlike classiﬁer-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.  Fast YOLO is the fastest general-purpose object detec- tor in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.  Acknowledgements: This work is partially supported by ONR N00014-13-1-0720, NSF'),\n",
              "  0.36881598426389084)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain Setup"
      ],
      "metadata": {
        "id": "Ynhq8ajuvId7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an LLM instance using Cohere's \"command-r\" model\n",
        "# The temperature parameter controls randomness in the generated responses; 0 ensures deterministic outputs\n",
        "llm = ChatCohere(model=\"command-r\", temperature=0)"
      ],
      "metadata": {
        "id": "lyw9g3RGc-f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a prompt template for generating answers based on a given context and question\n",
        "prompt_str = \"\"\"Given a chat history and the latest user question which might reference context in the chat history,\n",
        "formulate a standalone question which can be understood without the chat history. Do NOT answer the question,\n",
        "just reformulate it if needed and otherwise return it as is.\n",
        "\"\"\"\n",
        "\n",
        "# Use a prompt that includes a MessagesPlaceholder variable under the name \"chat_history\".\n",
        "# This allows us to pass in a list of Messages to the prompt using the \"chat_history\" input key,\n",
        "# and these messages will be inserted after the system message and before the human message containing the\n",
        "# latest question.\n",
        "\n",
        "prompt_history_aware = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", prompt_str),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    (\"human\", \"{input}\"),\n",
        "])\n",
        "\n",
        "# create_history_aware_retriever constructs a chain that accepts keys input and chat_history as input, and has the same output schema as a retriever\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, vectordb.as_retriever(), prompt_history_aware\n",
        ")"
      ],
      "metadata": {
        "id": "Iu5f5VHjc_7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our final rag_chain with create_retrieval_chain. This chain applies the history_aware_retriever and question_answer_chain created with create_stuff_documents_chain in sequence, retaining intermediate outputs such as the retrieved context for convenience. It has input keys input and chat_history, and includes input, chat_history, context, and answer in its output.\n",
        "\n"
      ],
      "metadata": {
        "id": "7KlAuEFug13y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question. If you don't know the answer,\n",
        "say that you don't know. Use three sentences maximum and keep theanswer concise.\n",
        "\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# create_retrieval_chain combines history aware retriever and the qa chain\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "rikK8JUEdG1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a empty chat_history list\n",
        "chat_history = []\n",
        "\n",
        "# Invoking the chain\n",
        "question = \"What is YOLO?\"\n",
        "response = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
        "\n",
        "# Appeding question and response answers\n",
        "chat_history.extend(\n",
        "    [\n",
        "        HumanMessage(content=question),\n",
        "        AIMessage(content=response[\"answer\"]),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "899TdBlaetnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check chat_history\n",
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhBg5UEhe4nT",
        "outputId": "90e60d23-d5ad-46f2-e861-898c772a0b76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is YOLO?'),\n",
              " AIMessage(content='YOLO is a real-time object detection system, which uses a single convolutional neural network to simultaneously predict multiple bounding boxes and class probabilities for those boxes. The name YOLO is an acronym for \"You Only Look Once\".')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_question = \"What are the tasks YOLO can be used in?\"\n",
        "response = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
        "\n",
        "print(response[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-VK65r1e-Tk",
        "outputId": "fb3626b7-6eed-4a1f-b6f9-8149b74177a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "YOLO is a fast and accurate object detector which can be applied to various tasks including person detection and tracking objects in real-time. It's also effective for use in computer vision applications and general purpose robotic systems due to its real-time performance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To automate the inserting and updating of chat history. And have a session id that can be unique for a user"
      ],
      "metadata": {
        "id": "YxMOMAC2hXeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ],
      "metadata": {
        "id": "xgrbYPqihi4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What is Transformers?\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"User_1\"}\n",
        "    },\n",
        ")[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "7Oj0m5wXh5b5",
        "outputId": "9cb5f919-ed1f-4e23-a134-03718b78fb20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Transformers are a type of neural network architecture that excels in sequence transduction tasks, such as machine translation and language modeling. They're called Transformers because they use self-attention mechanisms to transform input sequences into output sequences, hence the name. This architecture, first proposed in 2017, relies entirely on attention mechanisms and does not use recurrence or convolutions, which sets it apart from previous models.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This will output input, chat_history, contexts and answer\n",
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"Transformers vs YOLO\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"User_1\"}\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nt1XZwLxiLnV",
        "outputId": "4e40369b-b2a5-4a67-e345-4bde53786d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Transformers vs YOLO',\n",
              " 'chat_history': [HumanMessage(content='What is Transformers?'),\n",
              "  AIMessage(content=\"Transformers are a type of neural network architecture that excels in sequence transduction tasks, such as machine translation and language modeling. They're called Transformers because they use self-attention mechanisms to transform input sequences into output sequences, hence the name. This architecture, first proposed in 2017, relies entirely on attention mechanisms and does not use recurrence or convolutions, which sets it apart from previous models.\")],\n",
              " 'context': [Document(metadata={'source': '/content/1506.02640v5.pdf'}, page_content='You Only Look Once: Uniﬁed, Real-Time Object Detection  Joseph Redmon∗, Santosh Divvala∗†, Ross Girshick¶, Ali Farhadi∗† University of Washington∗, Allen Institute for AI†, Facebook AI Research¶ http://pjreddie.com/yolo/  6 1 0 2  y a M 9  ]  V C . s c [  5 v 0 4 6 2 0 . 6 0 5 1 : v i X r a  Abstract  We present YOLO, a new approach to object detection. Prior work on object detection repurposes classiﬁers to per- form detection. Instead, we frame object detection as a re- gression problem to spatially separated bounding boxes and associated class probabilities. A single neural network pre- dicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance.  Our uniﬁed architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detec- tors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other de- tection methods, including DPM and R-CNN, when gener- alizing from natural images to other domains like artwork.  1. Introduction  Humans glance at an image and instantly know what ob- jects are in the image, where they are, and how they inter- act. The human visual system is fast and accurate, allow- ing us to perform complex tasks like driving with little con- scious thought. Fast, accurate algorithms for object detec- tion would allow computers to drive cars without special- ized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.  Current detection systems repurpose classiﬁers to per- form detection. To'),\n",
              "  Document(metadata={'source': '/content/1506.02640v5.pdf'}, page_content='they are, and how they inter- act. The human visual system is fast and accurate, allow- ing us to perform complex tasks like driving with little con- scious thought. Fast, accurate algorithms for object detec- tion would allow computers to drive cars without special- ized sensors, enable assistive devices to convey real-time scene information to human users, and unlock the potential for general purpose, responsive robotic systems.  Current detection systems repurpose classiﬁers to per- form detection. To detect an object, these systems take a classiﬁer for that object and evaluate it at various locations and scales in a test image. Systems like deformable parts models (DPM) use a sliding window approach where the classiﬁer is run at evenly spaced locations over the entire image [10].  Figure 1: The YOLO Detection System. Processing images with YOLO is simple and straightforward. Our system (1) resizes the input image to 448 × 448, (2) runs a single convolutional net- work on the image, and (3) thresholds the resulting detections by the model’s conﬁdence.  methods to ﬁrst generate potential bounding boxes in an im- age and then run a classiﬁer on these proposed boxes. After classiﬁcation, post-processing is used to reﬁne the bound- ing boxes, eliminate duplicate detections, and rescore the boxes based on other objects in the scene [13]. These com- plex pipelines are slow and hard to optimize because each individual component must be trained separately.  We reframe object detection as a single regression prob- lem, straight from image pixels to bounding box coordi- nates and class probabilities. Using our system, you only look once (YOLO) at an image to predict what objects are present and where they are.  YOLO is refreshingly simple: see Figure 1. A sin- gle convolutional network simultaneously predicts multi- ple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detec- tion performance. This uniﬁed model has several beneﬁts over traditional methods of'),\n",
              "  Document(metadata={'source': '/content/1506.02640v5.pdf'}, page_content='Search [35] generates potential bounding boxes, a convolu- tional network extracts features, an SVM scores the boxes, a linear model adjusts the bounding boxes, and non-max sup- pression eliminates duplicate detections. Each stage of this complex pipeline must be precisely tuned independently and the resulting system is very slow, taking more than 40 seconds per image at test time [14].  YOLO shares some similarities with R-CNN. Each grid cell proposes potential bounding boxes and scores those boxes using convolutional features. However, our system puts spatial constraints on the grid cell proposals which helps mitigate multiple detections of the same object. Our system also proposes far fewer bounding boxes, only 98 per image compared to about 2000 from Selective Search. Finally, our system combines these individual components into a single, jointly optimized model.  Other Fast Detectors Fast and Faster R-CNN focus on speeding up the R-CNN framework by sharing computa- tion and using neural networks to propose regions instead of Selective Search [14] [28]. While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance.  Many research efforts focus on speeding up the DPM pipeline [31] [38] [5]. They speed up HOG computation, use cascades, and push computation to GPUs. However, only 30Hz DPM [31] actually runs in real-time.  Instead of trying to optimize individual components of a large detection pipeline, YOLO throws out the pipeline entirely and is fast by design.  Detectors for single classes like faces or people can be highly optimized since they have to deal with much less variation [37]. YOLO is a general purpose detector that learns to detect a variety of objects simultaneously.  Deep MultiBox. Unlike R-CNN, Szegedy et al. train a convolutional neural network to predict regions of interest [8] instead of using Selective Search. MultiBox can also perform single object detection by replacing the conﬁdence prediction with a single class prediction. However, Multi-'),\n",
              "  Document(metadata={'source': '/content/1506.02640v5.pdf'}, page_content='Detection In The Wild  Academic datasets for object detection draw the training and testing data from the same distribution. In real-world applications it is hard to predict all possible use cases and  YOLO is a fast, accurate object detector, making it ideal for computer vision applications. We connect YOLO to a webcam and verify that it maintains real-time performance,  \\x0cVOC 2007 AP 59.2 54.2 43.2 36.5 -  Picasso AP Best F1 0.590 53.3 0.226 10.4 0.458 37.8 0.271 17.8 0.051 1.9  People-Art AP 45 26 32  YOLO R-CNN DPM Poselets [2] D&T [4]  (a) Picasso Dataset precision-recall curves.  (b) Quantitative results on the VOC 2007, Picasso, and People-Art Datasets. The Picasso Dataset evaluates on both AP and best F1 score.  Figure 5: Generalization results on Picasso and People-Art datasets.  Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it does think one person is an airplane.  including the time to fetch images from the camera and dis- play the detections.  The resulting system is interactive and engaging. While YOLO processes images individually, when attached to a webcam it functions like a tracking system, detecting ob- jects as they move around and change in appearance. A demo of the system and the source code can be found on our project website: http://pjreddie.com/yolo/.  6. Conclusion  We introduce YOLO, a uniﬁed model for object detec- tion. Our model is simple to construct and can be trained  directly on full images. Unlike classiﬁer-based approaches, YOLO is trained on a loss function that directly corresponds to detection performance and the entire model is trained jointly.  Fast YOLO is the fastest general-purpose object detec- tor in the literature and YOLO pushes the state-of-the-art in real-time object detection. YOLO also generalizes well to new domains making it ideal for applications that rely on fast, robust object detection.  Acknowledgements: This work is partially supported by ONR N00014-13-1-0720, NSF')],\n",
              " 'answer': 'YOLO (You Only Look Once) is a real-time object detection system that uses a single convolutional neural network to predict bounding boxes and class probabilities directly from full images. It is a fast and accurate object detector that processes images in real-time.\\n\\nTransformers are a type of neural network architecture that excel in understanding long-range dependencies, which makes them perform well on tasks like machine translation and text summarization. They use self-attention mechanisms to process sequential data.\\n\\nTherefore, YOLO and Transformers are different approaches to solving distinct problems in computer vision and natural language processing, respectively.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the chat_history of the store dict\n",
        "for message in store[\"User_1\"].messages:\n",
        "    if isinstance(message, AIMessage):\n",
        "        prefix = \"AI\"\n",
        "    else:\n",
        "        prefix = \"User\"\n",
        "\n",
        "    print(f\"{prefix}: {message.content}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP-c6pmxh8IS",
        "outputId": "1fb68850-7809-409c-8171-d0b026f1e07a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: What is Transformers?\n",
            "\n",
            "AI: Transformers are a type of neural network architecture that excels in sequence transduction tasks, such as machine translation and language modeling. They're called Transformers because they use self-attention mechanisms to transform input sequences into output sequences, hence the name. This architecture, first proposed in 2017, relies entirely on attention mechanisms and does not use recurrence or convolutions, which sets it apart from previous models.\n",
            "\n",
            "User: Transformers vs YOLO\n",
            "\n",
            "AI: YOLO (You Only Look Once) is a real-time object detection system that uses a single convolutional neural network to predict bounding boxes and class probabilities directly from full images. It is a fast and accurate object detector that processes images in real-time.\n",
            "\n",
            "Transformers are a type of neural network architecture that excel in understanding long-range dependencies, which makes them perform well on tasks like machine translation and text summarization. They use self-attention mechanisms to process sequential data.\n",
            "\n",
            "Therefore, YOLO and Transformers are different approaches to solving distinct problems in computer vision and natural language processing, respectively.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}