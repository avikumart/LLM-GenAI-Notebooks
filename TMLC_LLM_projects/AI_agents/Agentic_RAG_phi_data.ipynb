{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3d43017bbfba4654b8acb9302e179b22": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_86f4bcb62fe94cba8fd00aa002910ce9",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m▰▰▱▱▱▱▱\u001b[0m Thinking...\n\u001b[36m┏━\u001b[0m\u001b[36m Message \u001b[0m\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[36m━┓\u001b[0m\n\u001b[36m┃\u001b[0m                                                                                                                 \u001b[36m┃\u001b[0m\n\u001b[36m┃\u001b[0m \u001b[32mDoes GPT-4 accept visual inputs?\u001b[0m                                                                                \u001b[36m┃\u001b[0m\n\u001b[36m┃\u001b[0m                                                                                                                 \u001b[36m┃\u001b[0m\n\u001b[36m┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\u001b[0m\n\u001b[34m┏━\u001b[0m\u001b[34m Response (4.9s) \u001b[0m\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[34m━┓\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m Running:                                                                                                        \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m • \u001b[0msearch_knowledge_base(query=Does GPT-4 accept visual inputs?)                                                \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m Yes, GPT-4 can accept visual inputs in addition to text. This capability allows the model to process prompts    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m that include both images and text, enabling it to perform a variety of tasks related to vision and language.    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                   \u001b[1mKey Points:\u001b[0m                                                   \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m • \u001b[0m\u001b[1mMultimodal Capability\u001b[0m: GPT-4 processes both text and images, which means it can understand and generate      \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0mresponses based on content found in images alongside textual inputs.                                         \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m • \u001b[0m\u001b[1mTask Versatility\u001b[0m: Users can specify any vision or language task with prompts that are interlaced with both   \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0mtext and images.                                                                                             \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m • \u001b[0m\u001b[1mExamples of Use\u001b[0m: GPT-4 has demonstrated its ability to interpret and provide insights on images, such as     \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0midentifying unusual elements in images or explaining memes.                                                  \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m • \u001b[0m\u001b[1mPerformance\u001b[0m: In terms of its performance, GPT-4 exhibits human-level capabilities in understanding these     \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0mmultimodal inputs across various domains.                                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m This enhancement in input handling marks a significant advancement compared to its predecessors, which only     \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m processed text.                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">▰▰▱▱▱▱▱</span> Thinking...\n<span style=\"color: #008080; text-decoration-color: #008080\">┏━ Message ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span> <span style=\"color: #008000; text-decoration-color: #008000\">Does GPT-4 accept visual inputs?</span>                                                                                <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┏━ Response (4.9s) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> Running:                                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>search_knowledge_base(query=Does GPT-4 accept visual inputs?)                                                <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> Yes, GPT-4 can accept visual inputs in addition to text. This capability allows the model to process prompts    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> that include both images and text, enabling it to perform a variety of tasks related to vision and language.    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                   <span style=\"font-weight: bold\">Key Points:</span>                                                   <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Multimodal Capability</span>: GPT-4 processes both text and images, which means it can understand and generate      <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>responses based on content found in images alongside textual inputs.                                         <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Task Versatility</span>: Users can specify any vision or language task with prompts that are interlaced with both   <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>text and images.                                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Examples of Use</span>: GPT-4 has demonstrated its ability to interpret and provide insights on images, such as     <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>identifying unusual elements in images or explaining memes.                                                  <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Performance</span>: In terms of its performance, GPT-4 exhibits human-level capabilities in understanding these     <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>multimodal inputs across various domains.                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> This enhancement in input handling marks a significant advancement compared to its predecessors, which only     <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> processed text.                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "86f4bcb62fe94cba8fd00aa002910ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7814340112e47ec88f50f308204414e": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3be7f54b48614c21bf77092bd3fbd18e",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m▰▰▰▰▰▰▰\u001b[0m Thinking...\n\u001b[36m┏━\u001b[0m\u001b[36m Message \u001b[0m\u001b[36m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[36m━┓\u001b[0m\n\u001b[36m┃\u001b[0m                                                                                                                 \u001b[36m┃\u001b[0m\n\u001b[36m┃\u001b[0m \u001b[32mWhat is the comparison of GPT-4 and GPT-3.5?\u001b[0m                                                                    \u001b[36m┃\u001b[0m\n\u001b[36m┃\u001b[0m                                                                                                                 \u001b[36m┃\u001b[0m\n\u001b[36m┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\u001b[0m\n\u001b[34m┏━\u001b[0m\u001b[34m Response (36.5s) \u001b[0m\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[34m━┓\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m Running:                                                                                                        \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m • \u001b[0msearch_knowledge_base(query=comparison of GPT-4 and GPT-3.5)                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m Here's a comparison between \u001b[1mGPT-4\u001b[0m and \u001b[1mGPT-3.5\u001b[0m, highlighting key differences and advancements:                   \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                 \u001b[1mKey Differences\u001b[0m                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 1 \u001b[0m\u001b[1mMultimodal Capabilities\u001b[0m:                                                                                     \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mGPT-4\u001b[0m: Can accept both text and visual inputs, allowing users to interlace images and text in prompts.    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mGPT-3.5\u001b[0m: Limited to processing text inputs only.                                                          \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 2 \u001b[0m\u001b[1mPerformance on Tasks\u001b[0m:                                                                                        \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mAccuracy\u001b[0m: GPT-4 outperforms GPT-3.5 across a range of benchmarks, including professional and academic     \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtests. For instance, on several standardized exams, GPT-4 scored significantly higher.                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mFine-Tuning\u001b[0m: GPT-4 underwent reinforcement learning from human feedback (RLHF), improving its alignment   \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mwith user intent and reducing the generation of toxic outputs compared to GPT-3.5.                        \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 3 \u001b[0m\u001b[1mResponse Quality\u001b[0m:                                                                                            \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mPreference tests showed that responses generated by GPT-4 were preferred over those from GPT-3.5 about    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0m70.2% of the time, indicating a marked improvement in response quality and relevance.                     \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 4 \u001b[0m\u001b[1mReduction of Unwanted Outputs\u001b[0m:                                                                               \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mToxicity & Hallucinations\u001b[0m: GPT-4 demonstrates a lower frequency of generating toxic content (6.48% of the \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mtime) and has undergone specific strategies to minimize hallucinations compared to GPT-3.5.               \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 5 \u001b[0m\u001b[1mComplex Task Handling\u001b[0m:                                                                                       \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0m\u001b[1mExamples\u001b[0m: GPT-4 handles complex tasks better, providing more nuanced and contextually appropriate         \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0mresponses, especially in ambiguous or detailed scenarios.                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m 6 \u001b[0m\u001b[1mSafety and Alignment Improvements\u001b[0m:                                                                           \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m • \u001b[0mAdditional efforts were made in GPT-4 to enhance safety and reduce biases, utilizing domain experts for   \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m \u001b[1;33m   \u001b[0m\u001b[1;33m   \u001b[0madversarial testing which led to improved safety metrics over GPT-3.5.                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                  \u001b[1mSummary Table\u001b[0m                                                  \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m  \u001b[1m \u001b[0m\u001b[1mFeature/Capability\u001b[0m\u001b[1m   \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGPT-3.5\u001b[0m\u001b[1m  \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1mGPT-4\u001b[0m\u001b[1m                       \u001b[0m\u001b[1m \u001b[0m                                             \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                             \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m   Input Type              Text Only   Text & Image                                                              \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m   Benchmark Performance   Lower       Higher                                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m   RLHF Fine-Tuning        Limited     Extensive                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m   Toxic Output Rate       Higher      6.48%                                                                     \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m   Preferred Responses     Baseline    70.2% preferred over GPT-3.5                                              \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m   Handling Complexity     Basic       Advanced                                                                  \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m   Safety Mechanisms       Basic       Enhanced                                                                  \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                   \u001b[1mConclusion\u001b[0m                                                    \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m Overall, GPT-4 represents a significant advancement over GPT-3.5, particularly in its ability to process        \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m multimodal inputs, improve response quality, and implement safety and alignment strategies.                     \u001b[34m┃\u001b[0m\n\u001b[34m┃\u001b[0m                                                                                                                 \u001b[34m┃\u001b[0m\n\u001b[34m┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">▰▰▰▰▰▰▰</span> Thinking...\n<span style=\"color: #008080; text-decoration-color: #008080\">┏━ Message ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span> <span style=\"color: #008000; text-decoration-color: #008000\">What is the comparison of GPT-4 and GPT-3.5?</span>                                                                    <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┃</span>                                                                                                                 <span style=\"color: #008080; text-decoration-color: #008080\">┃</span>\n<span style=\"color: #008080; text-decoration-color: #008080\">┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┏━ Response (36.5s) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> Running:                                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>search_knowledge_base(query=comparison of GPT-4 and GPT-3.5)                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> Here's a comparison between <span style=\"font-weight: bold\">GPT-4</span> and <span style=\"font-weight: bold\">GPT-3.5</span>, highlighting key differences and advancements:                   <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                 <span style=\"font-weight: bold\">Key Differences</span>                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span><span style=\"font-weight: bold\">Multimodal Capabilities</span>:                                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">GPT-4</span>: Can accept both text and visual inputs, allowing users to interlace images and text in prompts.    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">GPT-3.5</span>: Limited to processing text inputs only.                                                          <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span><span style=\"font-weight: bold\">Performance on Tasks</span>:                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Accuracy</span>: GPT-4 outperforms GPT-3.5 across a range of benchmarks, including professional and academic     <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>tests. For instance, on several standardized exams, GPT-4 scored significantly higher.                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Fine-Tuning</span>: GPT-4 underwent reinforcement learning from human feedback (RLHF), improving its alignment   <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>with user intent and reducing the generation of toxic outputs compared to GPT-3.5.                        <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span><span style=\"font-weight: bold\">Response Quality</span>:                                                                                            <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Preference tests showed that responses generated by GPT-4 were preferred over those from GPT-3.5 about    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>70.2% of the time, indicating a marked improvement in response quality and relevance.                     <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span><span style=\"font-weight: bold\">Reduction of Unwanted Outputs</span>:                                                                               <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Toxicity &amp; Hallucinations</span>: GPT-4 demonstrates a lower frequency of generating toxic content (6.48% of the <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>time) and has undergone specific strategies to minimize hallucinations compared to GPT-3.5.               <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 5 </span><span style=\"font-weight: bold\">Complex Task Handling</span>:                                                                                       <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span><span style=\"font-weight: bold\">Examples</span>: GPT-4 handles complex tasks better, providing more nuanced and contextually appropriate         <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>responses, especially in ambiguous or detailed scenarios.                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 6 </span><span style=\"font-weight: bold\">Safety and Alignment Improvements</span>:                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">    • </span>Additional efforts were made in GPT-4 to enhance safety and reduce biases, utilizing domain experts for   <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">      </span>adversarial testing which led to improved safety metrics over GPT-3.5.                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                  <span style=\"font-weight: bold\">Summary Table</span>                                                  <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>  <span style=\"font-weight: bold\"> Feature/Capability    </span> <span style=\"font-weight: bold\"> GPT-3.5   </span> <span style=\"font-weight: bold\"> GPT-4                        </span>                                             <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>  ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━                                             <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>   Input Type              Text Only   Text &amp; Image                                                              <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>   Benchmark Performance   Lower       Higher                                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>   RLHF Fine-Tuning        Limited     Extensive                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>   Toxic Output Rate       Higher      6.48%                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>   Preferred Responses     Baseline    70.2% preferred over GPT-3.5                                              <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>   Handling Complexity     Basic       Advanced                                                                  <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>   Safety Mechanisms       Basic       Enhanced                                                                  <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                   <span style=\"font-weight: bold\">Conclusion</span>                                                    <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> Overall, GPT-4 represents a significant advancement over GPT-3.5, particularly in its ability to process        <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span> multimodal inputs, improve response quality, and implement safety and alignment strategies.                     <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┃</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">┃</span>\n<span style=\"color: #000080; text-decoration-color: #000080\">┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "3be7f54b48614c21bf77092bd3fbd18e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/TMLC_LLM_projects/AI_agents/Agentic_RAG_phi_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "gAAbM7soZFiR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_1J2KSqfB4GI",
        "outputId": "947a1e56-e38d-4fa5-89c4-5f0453e68c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m716.9/716.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.3/32.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install phidata openai duckduckgo-search lancedb tantivy pypdf -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Libraries and API keys"
      ],
      "metadata": {
        "id": "qi5XmX_xZIYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "OpenAI_API = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = OpenAI_API"
      ],
      "metadata": {
        "id": "gc-SUK5ECGC6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from phi.knowledge.pdf import PDFKnowledgeBase, PDFReader\n",
        "from phi.vectordb.lancedb import LanceDb\n",
        "from phi.vectordb.search import SearchType\n",
        "from phi.agent import Agent\n",
        "from phi.storage.agent.sqlite import SqlAgentStorage\n",
        "from phi.model.openai import OpenAIChat\n",
        "from phi.tools.duckduckgo import DuckDuckGo"
      ],
      "metadata": {
        "id": "B6VM3eOhCIEb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VectorDB setup"
      ],
      "metadata": {
        "id": "oUUy_ZcfZPQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a LanceDb object for vector database operations\n",
        "vector_db = LanceDb(\n",
        "    table_name=\"documents\",  # Name of the table to store vector data\n",
        "    uri=\"/tmp/lancedb\",    # File path for the database storage\n",
        "    search_type=SearchType.keyword,\n",
        ")"
      ],
      "metadata": {
        "id": "QXaYUMPACPrP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory named 'Data' in the current working directory\n",
        "! mkdir Data\n",
        "\n",
        "# Download the GPT-4 paper PDF from the given URL\n",
        "# Save the downloaded file into the 'Data' directory with the name 'gpt-4.pdf'\n",
        "! wget \"https://cdn.openai.com/papers/gpt-4.pdf\" -O Data/gpt-4.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-Rl9XQhCrJq",
        "outputId": "d55b0cc8-79d0-48e0-e8c3-22e1f0656cba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-10 06:20:47--  https://cdn.openai.com/papers/gpt-4.pdf\n",
            "Resolving cdn.openai.com (cdn.openai.com)... 13.107.253.69, 2620:1ec:29:1::69\n",
            "Connecting to cdn.openai.com (cdn.openai.com)|13.107.253.69|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5229908 (5.0M) [application/pdf]\n",
            "Saving to: ‘Data/gpt-4.pdf’\n",
            "\n",
            "Data/gpt-4.pdf      100%[===================>]   4.99M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-02-10 06:20:47 (55.0 MB/s) - ‘Data/gpt-4.pdf’ saved [5229908/5229908]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a PDFKnowledgeBase object for managing knowledge extracted from PDF files\n",
        "pdf_knowledge_base = PDFKnowledgeBase(\n",
        "    path=\"/content/Data/\",               # Path to the directory containing PDF files\n",
        "    vector_db=vector_db,                 # Vector database instance for storing and querying extracted information\n",
        "    reader=PDFReader(chunk=True),        # PDFReader instance with chunking enabled to process documents in smaller parts\n",
        ")"
      ],
      "metadata": {
        "id": "bJHlb18zCnlT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_knowledge_base.load() # load the data into vector db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "id": "vYP1x1zwC0eK",
        "outputId": "78597485-954c-481d-9ebb-0aee9752798b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[34mINFO    \u001b[0m Creating collection                                                                                       \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Creating collection                                                                                       \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[34mINFO    \u001b[0m Loading knowledge base                                                                                    \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Loading knowledge base                                                                                    \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[34mINFO    \u001b[0m Reading: gpt-\u001b[1;36m4\u001b[0m                                                                                            \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Reading: gpt-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>                                                                                            \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[34mINFO    \u001b[0m Added \u001b[1;36m104\u001b[0m documents to knowledge base                                                                     \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Added <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">104</span> documents to knowledge base                                                                     \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Setup"
      ],
      "metadata": {
        "id": "gEXhkRmhZR83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    model=OpenAIChat(id=\"gpt-4o-mini\"),\n",
        "    knowledge=pdf_knowledge_base,\n",
        "    tools=[DuckDuckGo()],\n",
        "    show_tool_calls=True,\n",
        "    markdown=True,\n",
        "    storage=SqlAgentStorage(table_name=\"data\", db_file=\"data.db\"),\n",
        "    add_history_to_messages=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Wg9ZgfL3DVf0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.print_response(\n",
        "  \"Does GPT-4 accept visual inputs?\",\n",
        "  stream=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512,
          "referenced_widgets": [
            "3d43017bbfba4654b8acb9302e179b22",
            "86f4bcb62fe94cba8fd00aa002910ce9"
          ]
        },
        "id": "2JpknQ9MEa2q",
        "outputId": "e81ae2b2-41ae-489f-82fb-965b61ec9ac1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d43017bbfba4654b8acb9302e179b22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.print_response(\n",
        "  \"What is the comparison of GPT-4 and GPT-3.5?\",\n",
        "  stream=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958,
          "referenced_widgets": [
            "f7814340112e47ec88f50f308204414e",
            "3be7f54b48614c21bf77092bd3fbd18e"
          ]
        },
        "id": "dFlGgY-jC158",
        "outputId": "d49f6c49-83ae-4f2d-bac4-aa50cc0285da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7814340112e47ec88f50f308204414e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In both the questions, we can identify what is the source of information has been used to generate the response. Second query required to extract information from web while for first query information was present in the vectordb."
      ],
      "metadata": {
        "id": "uDeoHTlOXjKB"
      }
    }
  ]
}