{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/TMLC_LLM_projects/RAG/RAG_Personal_Resource_Assistant_Langchain%2C_Cohere_and_ChromaDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing necessary libraries"
      ],
      "metadata": {
        "id": "8UblPEH_V5FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cohere provides free trial keys to use their LLMs. So generate one trial key from dashboard.cohere.com"
      ],
      "metadata": {
        "id": "MHAhn6gkWIbK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8HmgD6eLRrzA",
        "outputId": "c0546d65-248f-4037-8f41-f3eb46b0c70e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.5/605.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.7/250.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain-cohere langchain pdfminer.six chromadb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "langchain-cohere: Enables integration of Cohere's language models with LangChain for advanced text generation and processing workflows.\n",
        "\n",
        "langchain: Provides a modular framework for building language model-powered applications, such as chatbots, question-answering systems, and conversational agents.\n",
        "\n",
        "pdfminer.six: Facilitates text extraction from PDF files, making it useful for document analysis and preprocessing tasks.\n",
        "\n",
        "chromadb: A vector database library designed for efficient storage and retrieval of embeddings, ideal for tasks like semantic search and recommendation systems."
      ],
      "metadata": {
        "id": "dYqZZciyV12N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "FAFkZ2UdV71s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_KEY')\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from pdfminer.high_level import extract_text as extract_text_pdf_miner\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_core.runnables import RunnableParallel,RunnablePassthrough"
      ],
      "metadata": {
        "id": "RpRhM8WhR2Vc",
        "outputId": "3953306f-40e6-43d4-a362-575c08c8c929",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VectorDB setup"
      ],
      "metadata": {
        "id": "DnFFjExUWAzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where the Chroma database will persist data\n",
        "persist_directory = \"/content/chroma_db\"\n",
        "\n",
        "# Initialize Cohere embeddings with the specified model\n",
        "# \"embed-english-v3.0\" is a pre-trained English language embedding model by Cohere\n",
        "# The user_agent parameter specifies the tool or library using the Cohere API, in this case, LangChain\n",
        "embedding = CohereEmbeddings(\n",
        "    model=\"embed-english-v3.0\",\n",
        "    user_agent=\"langchain\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "92kuclutTc9j",
        "outputId": "1f8452cb-bdba-4ed3-b185-dfadf2c8595f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-75fce1a602a5>:7: LangChainDeprecationWarning: The class `CohereEmbeddings` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereEmbeddings``.\n",
            "  embedding = CohereEmbeddings(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are processing 2 research papers on transformers and yolo. You can use the PDFs"
      ],
      "metadata": {
        "id": "68hrmDRbWmD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through a list of PDF files to process\n",
        "for pdf_name in [\"/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf\", \"/content/Newwhitepaper_Solving Domain-Specific problems using LLMs.pdf\"]:\n",
        "    # Open each PDF file in binary mode\n",
        "    with open(pdf_name, 'rb') as f:\n",
        "        # Extract text from the PDF using the extract_text_pdf_miner function\n",
        "        text = extract_text_pdf_miner(f)\n",
        "\n",
        "        # Clean the extracted text by removing newline characters and joining into a single string\n",
        "        cleaned_text = \" \".join(text.split(\"\\n\"))\n",
        "\n",
        "        # Initialize a list to store document chunks\n",
        "        docs = []\n",
        "\n",
        "        # Create a text splitter to divide the text into manageable chunks\n",
        "        # Each chunk has a maximum size of 2048 characters with a 512-character overlap\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
        "\n",
        "        # Split the cleaned text into chunks and wrap each chunk in a Document object\n",
        "        for chunk in splitter.split_text(cleaned_text):\n",
        "            docs.append(Document(page_content=chunk, metadata={\"source\": pdf_name}))\n",
        "\n",
        "    # Create a Chroma collection from the processed documents\n",
        "    # Use the specified persist directory and embedding model for storage and retrieval\n",
        "    vector_collection_fixed_size = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        persist_directory=persist_directory,\n",
        "        embedding=embedding\n",
        "    )"
      ],
      "metadata": {
        "id": "5lGwexERTCCx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Chroma vector database\n",
        "# The persist_directory specifies the location where the database is stored\n",
        "# The embedding_function parameter provides the embedding model used for vector representation\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wmFXQin5Ulz9",
        "outputId": "49ba7f33-ad8d-461e-d71f-6a2eadb28cd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-66dc66a8b09c>:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb"
      ],
      "metadata": {
        "id": "BCEP9U0qXAvm",
        "outputId": "7210b2c1-1f9f-444b-b250-8dd1a360765c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.chroma.Chroma at 0x7de5441a88b0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a similarity search on the vector database\n",
        "# The query \"What is YOLO?\" is used to find the most relevant documents\n",
        "# k=1 specifies that the top 1 most similar document should be retrieved\n",
        "# The method also returns relevance scores indicating how closely each document matches the query\n",
        "vectordb.similarity_search_with_relevance_scores(\"how to operationalize the genAI using vertex?\", k=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykJDpWIXU8r9",
        "outputId": "da538903-3fc6-4c2a-bd45-5e126c8d3677"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(metadata={'source': '/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf'}, page_content='Operationalizing  Generative AI on  Vertex AI using  MLOps  Authors: Anant Nawalgaria,   Gabriela Hernandez Larios, Elia Secchi,   Mike Styer, Christos Aniftos   and Onofrio Petragallo  \\x0cAcknowledgements  Reviewers and Contributors  Nenshad Bardoliwalla  Warren Barkley  Mikhail Chrestkha  Chase Lyall  Lakshmanan Sethu  Erwan Menard  Curators and Editors  Antonio Gulli  Anant Nawalgaria  Grace Mollison   Technical Writer  Joey Haymaker  Designer  Michael Lanning   2  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cTable of contents  Introduction     What are DevOps and MLOps?   Lifecycle of a gen AI system   Discover   Develop and experiment   The foundational model paradigm   The core component of LLM Systems: A prompted model component     Chain & Augment   Tuning & training     Data Practices   Evaluate   Deploy     Deployment of gen AI systems   Version control     Continuous integration of gen AI systems     Continuous delivery of gen AI systems     Deployment of foundation models   Infrastructure validation   5  6  7  9  10  11  13  16  20  23  27  30  31  31  32  33  34  34                        \\x0c  Compression and optimization     Deployment, packaging, and serving checklist   Logging and monitoring     Govern   The role of an AI platform for gen AI operations   Key components of Vertex AI for gen AI     Discover: Vertex Model Garden   Prototype: Vertex AI Studio & Notebooks     Customize: Vertex AI training & tuning    Train    Tune      Orchestrate     Chain & Augment: Vertex AI Grounding, Extensions, and RAG building blocks   Evaluate: Vertex AI Experiments, Tensorboard, & evaluation pipelines   Experiment   Evaluation   Predict: Vertex AI endpoints & monitoring     Govern: Vertex AI Feature Store, Model Registry, and Dataplex   Conclusion   Endnotes   35  36  37  41  42  43  44  47  48  49  49  51  52  55  56  57  57  59  61  63                                  \\x0cEmergence of foundation models  and generative AI (gen AI) has  introduced a new era for building  AI systems.'),\n",
              "  0.4152834100573892)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an LLM instance using Cohere's \"command-r\" model\n",
        "# The temperature parameter controls randomness in the generated responses; 0 ensures deterministic outputs\n",
        "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
        "\n",
        "# Define a prompt template for generating answers based on a given context and question\n",
        "prompt_str = \"\"\"Answer the question below using the context:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "# Create a ChatPromptTemplate from the string template, enabling dynamic input for context and question\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "# Create a retrieval pipeline to fetch relevant context and pass through the user's question\n",
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        # Use the vector database as a retriever to fetch relevant context for the question\n",
        "        \"context\": vectordb.as_retriever(),\n",
        "\n",
        "        # Pass through the user's input question without modification\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")\n",
        "\n",
        "# Define an output parser to format the generated response into a string\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Create a processing chain that retrieves context, formats the prompt, generates an LLM response, and parses the output\n",
        "chain = retrieval | prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "7mUIP130UsTf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke the chain of components (retrieval, prompt generation, LLM processing, and output parsing)\n",
        "# The question \"What is YOLO?\" is passed through the chain to generate the response\n",
        "response = chain.invoke(\"What is LLMs?\")\n",
        "\n",
        "# Print the response generated by the chain\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydFLc8lbVG9V",
        "outputId": "a5a9c06b-036a-4992-a774-8b944ab7fcca"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLMs are large language models which have become powerful tools to tackle complex challenges in different domains. While the early focus was on general-purpose tasks, LLMs have evolved to address specific problems in specialized fields through fine-tuning. They process vast amounts of data across multiple domains, enhancing existing workflows and unlocking new possibilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other chain invoking methods!\n",
        "\n",
        ".invoke(): The goal is to pass in an input and receive the output—neither more nor less.\n",
        "\n",
        ".batch(): This is faster than using invoke three times when you wish to supply several inputs to get multiple outputs because it handles the parallelization for you.\n",
        "\n",
        ".stream():  We may begin printing the response before the entire response is complete."
      ],
      "metadata": {
        "id": "cMhCRX0mZAnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_with_batch = chain.batch([\"What is Transformers\", \"How is Transformer different than LLMs?\"])\n",
        "\n",
        "for response in response_with_batch:\n",
        "  print(response)\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G19NkErGZ-MW",
        "outputId": "931e2824-990d-481c-f982-8197144939fd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I found several references to the term 'Transformer' in the documents you provided. \n",
            "\n",
            "The Vision Transformer (ViT) is a type of documentation focused on transformers in the field of AI. Mingxing Tan and Quoc V. Le also refer to 'EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks', which employs transformers. Hugging Face also has a documentation site for transformers called 'Transformers Documentation'. \n",
            "\n",
            "However, I found no detailed explanation of what transformers are or their purpose in the provided texts. I can infer that they are a technology used in machine learning models, but I cannot provide a full and precise definition in this instance.\n",
            "\n",
            "The term 'transform' also appears in the document, where it discusses the transformation of real-world data patterns over time and the need for models to adapt and learn from these changes.\n",
            "\n",
            "\n",
            "The term \"Transformer\" refers to a type of neural network architecture, while LLMs (Large Language Models) are a category of models that can be based on the Transformer architecture among other architectures. Transformer is a more narrow concept, referring to a specific type of network design, whereas LLMs are a broader category of models designed for understanding and generating human language, which can be based on different architectures, including the Transformer.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in chain.stream(\"What are the 3 vectors in Transformers architecture?\"):\n",
        "  print(chunk, flush=True, end=\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c6fKahxaoZb",
        "outputId": "eb131168-4b41-45c6-98c2-62d46f897804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I found no mention of three vectors in the Transformer architecture. However, a key component in the Transformer architecture is the use of three distinct sub-layers in both the encoder and decoder stacks. \n",
            "\n",
            "The first sub-layer employs a multi-head self-attention mechanism, which presumably involves multiple attention heads that operate in parallel and attend to different portions of the input sequence. This allows the model to capture different aspects of input dependencies.\n",
            "\n",
            "The second sub-layer is a simple, position-wise fully connected feed-forward network, which applies the same transformation to all positions in the sequence. \n",
            "\n",
            "The third element, not a vector but rather a fundamental aspect of the architecture, is the residual connection applied around each sub-layer. This is followed by layer normalization to stabilize and streamline the information flow. \n",
            "\n",
            "These three architectural elements, in combination, form the key building blocks of the Transformer model, enabling it to capture dependencies in input sequences and generate appropriate output sequences."
          ]
        }
      ]
    }
  ]
}