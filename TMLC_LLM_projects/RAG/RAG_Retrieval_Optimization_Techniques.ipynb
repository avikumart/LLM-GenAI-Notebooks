{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/TMLC_LLM_projects/RAG/RAG_Retrieval_Optimization_Techniques.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "au4wxc3KTtmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "45e64534-8008-4943-ccad-f3e6e3d64b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-cohere\n",
            "  Downloading langchain_cohere-0.3.4-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.6.2-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting cohere<6.0,>=5.5.6 (from langchain-cohere)\n",
            "  Downloading cohere-5.13.6-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.27 (from langchain-cohere)\n",
            "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-experimental<0.4.0,>=0.3.0 (from langchain-cohere)\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere) (2.10.3)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langchain-cohere) (0.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (3.4.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.5-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.29.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.29.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.21.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
            "  Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere<6.0,>=5.5.6->langchain-cohere) (2.27.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.5.6->langchain-cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-cohere) (1.33)\n",
            "Collecting langchain-community<0.4.0,>=0.3.0 (from langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading langchain_community-0.3.14-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.50b0)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.3->langchain-cohere) (2024.2)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-cohere) (0.7.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-cohere) (3.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.14-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading marshmallow-3.24.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental<0.4.0,>=0.3.0->langchain-cohere)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_cohere-0.3.4-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.6.2-py3-none-any.whl (606 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.2/606.2 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading cohere-5.13.6-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.2/250.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.5-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading fastavro-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.14-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.3.14-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading protobuf-5.29.2-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.24.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=b8c939a8573260213d35b4a274b89570d94eabe07ac798480213fcbd2f11719c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, types-requests, rank_bm25, python-dotenv, pyproject_hooks, protobuf, parameterized, overrides, opentelemetry-util-http, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, fastavro, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, starlette, posthog, opentelemetry-proto, coloredlogs, build, pydantic-settings, pdfminer.six, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, dataclasses-json, opentelemetry-instrumentation, langchain-core, cohere, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, langchain, langchain-community, chromadb, langchain-experimental, langchain-cohere\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.25\n",
            "    Uninstalling langchain-core-0.3.25:\n",
            "      Successfully uninstalled langchain-core-0.3.25\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.12\n",
            "    Uninstalling langchain-0.3.12:\n",
            "      Successfully uninstalled langchain-0.3.12\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.2 cohere-5.13.6 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.9 fastapi-0.115.6 fastavro-1.10.0 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-31.0.0 langchain-0.3.14 langchain-cohere-0.3.4 langchain-community-0.3.14 langchain-core-0.3.29 langchain-experimental-0.3.4 marshmallow-3.24.1 mmh3-5.0.1 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.20.1 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-util-http-0.50b0 overrides-7.7.0 parameterized-0.9.0 pdfminer.six-20240706 posthog-3.7.5 protobuf-5.29.2 pydantic-settings-2.7.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 rank_bm25-0.2.2 starlette-0.41.3 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-cohere langchain pdfminer.six chromadb rank_bm25"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_KEY')\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from pdfminer.high_level import extract_text as extract_text_pdf_miner\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_core.runnables import RunnableParallel,RunnablePassthrough"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37xhdj5YjIto",
        "outputId": "dfa93073-6126-4eb7-f167-d843511bb4b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hybrid Search"
      ],
      "metadata": {
        "id": "JsdpSFFRixIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where the Chroma database will persist data\n",
        "persist_directory = \"/content/hybrid-search\"\n",
        "\n",
        "# Initialize Cohere embeddings with the specified model\n",
        "# \"embed-english-v3.0\" is a pre-trained English language embedding model by Cohere\n",
        "# The user_agent parameter specifies the tool or library using the Cohere API, in this case, LangChain\n",
        "embedding = CohereEmbeddings(\n",
        "    model=\"embed-english-v3.0\",\n",
        "    user_agent=\"langchain\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "FGNwUhNvjbun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87d5d53e-357c-412e-9e9f-8c195e12fed5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-3bb00f7d6c8c>:7: LangChainDeprecationWarning: The class `CohereEmbeddings` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereEmbeddings``.\n",
            "  embedding = CohereEmbeddings(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_to_vectordb(file_path,source):\n",
        "  global bm25_retriever\n",
        "  # Loop through a list of PDF files to process\n",
        "  pages = []\n",
        "\n",
        "  for pdf_name in [file_path]:\n",
        "      # Open each PDF file in binary mode\n",
        "      with open(pdf_name, 'rb') as f:\n",
        "          # Extract text from the PDF using the extract_text_pdf_miner function\n",
        "          text = extract_text_pdf_miner(f)\n",
        "\n",
        "          # Clean the extracted text by removing newline characters and joining into a single string\n",
        "          cleaned_text = \" \".join(text.split(\"\\n\"))\n",
        "\n",
        "          # Initialize a list to store document chunks\n",
        "          docs = []\n",
        "\n",
        "          # Create a text splitter to divide the text into manageable chunks\n",
        "          # Each chunk has a maximum size of 2048 characters with a 512-character overlap\n",
        "          splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
        "\n",
        "          # Split the cleaned text into chunks and wrap each chunk in a Document object\n",
        "          for chunk in splitter.split_text(cleaned_text):\n",
        "              docs.append(Document(page_content=chunk, metadata={\"retrived_from\":source,\"source\": pdf_name}))\n",
        "              pages.append(Document(page_content=chunk, metadata={\"retrived_from\":source,\"source\": pdf_name}))\n",
        "      # Create a Chroma collection from the processed documents\n",
        "      # Use the specified persist directory and embedding model for storage and retrieval\n",
        "      if source == 1:\n",
        "        bm25_retriever = BM25Retriever.from_documents(pages)\n",
        "      else:\n",
        "        db = Chroma.from_documents(\n",
        "            documents=docs,\n",
        "            persist_directory=persist_directory,\n",
        "            embedding=embedding\n",
        "        )\n",
        "\n",
        "load_data_to_vectordb(file_path=\"/content/Newwhitepaper_Agents.pdf\",source=1)\n",
        "load_data_to_vectordb(file_path=\"/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf\",source=2)"
      ],
      "metadata": {
        "id": "-JzB6c1Rjcn4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the BM25 retriever\n",
        "bm25_retriever.k = 2  # Retrieve top 2 results\n",
        "print(\"type of bm25\", type(bm25_retriever))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K85f6lm9i_d4",
        "outputId": "e5b6dc04-587d-4d76-d646-c0e34c5c6cb4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type of bm25 <class 'langchain_community.retrievers.bm25.BM25Retriever'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize retriever\n",
        "docsearch = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n",
        "retriever_chromadb = docsearch.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# Initialize the ensemble retriever\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, retriever_chromadb], weights=[0.3, 0.7]\n",
        ")"
      ],
      "metadata": {
        "id": "PogehneekHyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bab1c36-7c8d-4c2a-eea4-e806606c9057"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-641eb3961709>:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  docsearch = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example query\n",
        "query = \"What is a AI agents?\"\n",
        "\n",
        "# Retrieve relevant documents/products\n",
        "docs = ensemble_retriever.invoke(query)\n",
        "\n",
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ibG6dac4kRE4",
        "outputId": "38a882f3-0286-488d-969f-6c10b1d33587"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'retrived_from': 2, 'source': '/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf'}, page_content='AI Agent Builder, you are be able to easily ground your agents by pointing to a diverse range   of data sources, including structured datastores such us BigQuery, Spanner, Cloud SQL,   unstructured sources like website content crawling and cloud storage as well as connectors   to Google drive and other APIs. Agent Builder utilizes a robust foundation of Google Search   technologies, encompassing semantic search, content chunking, ranking, algorithms,   and user intent understanding. Under the hood it optimizes document loading, chunking,   embedding models, and ranking strategies. It abstracts away these complexities and allows   users to simply specify their data source to initiate the gen AI-powered agent.This approach   is ideal for organizations seeking to build robust search experiences for standard use cases   without extensive technical expertise.   Vector databases are specialized systems for managing multi-dimensional data. This data,   encompassing images, text, audio, video, and other structured or unstructured formats,   is represented as vectors capturing its semantic meaning. Vector databases accelerate   searching and retrieval within these high-dimensional spaces, enabling efficient tasks like   finding similar images from billions or extracting relevant text snippets based on various   53  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cinputs. For a deeper dive into these topics, refer to 4 and 19. Vertex AI offers three flexible   solutions for storing and serving embeddings at scale, catering to diverse use cases and   user profiles.  Vertex AI Vector Search7 is a highly scalable low-latency similarity search and fully  managed vector database scaling to billions of vector embeddings with auto-scaling. This   technology, built upon ScaNN72 (a Google-developed technology used in products like   Search, YouTube, and Play), allows you to search from billions of semantically similar or   related items within your stored data. In the context of gen AI, the most common use'),\n",
              " Document(metadata={'retrived_from': 2, 'source': '/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf'}, page_content=\"what arguments   to use.  Vertex AI Grounding5 helps users connect large models with verifiable information by  grounding them to internal data corpora on Vertex AI Agent Builder70 or external sources   using Google Search. This enables two key functionalities: verifying model-generated outputs   against internal or external sources and creating RAG systems using Google’s advanced   search capabilities that produce quality content grounded in your own or web search data.   52  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cVertex AI extensions6 let developers integrate Vertex Foundation Models with real-time  data and real-world actions through APIs and functions, enabling task execution and allowing   enhanced capabilities. This extends to leveraging 1st party extensions like Vertex AI Search7   and Code Interpreter,71 or 3rd party extensions for triggering and completing transactions.   Imagine building an application that leverages the LLM's knowledge to plan a trip and   seamlessly utilizes internal APIs to book hotels and flights, all within a single interface.   Additionally, Vertex Extensions facilitate function calling with the gemini-pro model, enabling   you to generate descriptions, pass them to the large model, receive JSON with function   arguments, and automatically call the function.  Vertex AI Agent Builder70 is an out-of-the-box solution that allows you to quickly build gen  AI agents, to be used as conversational chatbots or as part of a search engine. With Vertex   AI Agent Builder, you are be able to easily ground your agents by pointing to a diverse range   of data sources, including structured datastores such us BigQuery, Spanner, Cloud SQL,   unstructured sources like website content crawling and cloud storage as well as connectors   to Google drive and other APIs. Agent Builder utilizes a robust foundation of Google Search   technologies, encompassing semantic search, content chunking, ranking, algorithms,   and user intent understanding. Under the hood it\"),\n",
              " Document(metadata={'retrived_from': 2, 'source': '/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf'}, page_content='is required. Depending on the use case, leveraging only one prompted model to perform   a particular generation might not be sufficient. To solve this issue, leveraging a divide and   conquer approach, several prompted models can be connected together, along with calls   to external APIs and logic expressed as code. A sequence of prompted model components   connected together in this way is commonly known as a chain.   Figure 6.  Components of a chain and relative development process  16  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cTwo common chain-based patterns that have emerged to mitigate recency and   hallucinations are retrieval augmented generation (RAG)3 and Agents.   •  RAG addresses these challenges by augmenting pre-trained models with   “knowledge” retrieved from databases, bypassing the need for pre-training. This   enables grounding and reduces hallucinations by incorporating up-to-date factual   information directly into the generation process.   •  Agents, popularized by the ReAct prompting technique,4 leverage LLMs as mediators   interacting with various tools, including RAG systems, internal or external APIs,   custom extensions, or even with other agents. This enables complex queries and   real-time actions by dynamically selecting and utilizing relevant information sources.   The LLM, acting as an agent, interprets the user’s query, decides which tool to utilize,   and how to formulate the response based on the retrieved information.  RAG and Agents approaches can be combined to create multi-agent systems connected   to large information networks, enabling sophisticated query handling and real-time   decision-making.   The orchestration of different models, logic and APIs is not a novelty of gen AI   Applications. For example, recommendation engines have long combined collaborative   filtering models, content-based models, and business rules to generate personalized   product recommendations for users. Similarly, in fraud detection, machine learning   models are'),\n",
              " Document(metadata={'retrived_from': 2, 'source': '/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf'}, page_content='and models used and   their lineage, the code involved and the relative evaluation data and metrics. This can help   auditing, debugging and improvements of the models  Along with these new practices, existing MLOps and DevOps practices still apply to MLOps   for gen AI:  1.  The need to govern the data lifecycle; see “Data Practices”.  2.  The need to govern the tuned model lifecycle; see “Tuning and Training”.  3.  The need to govern the code lifecycle; see “Deployment of GenAI   System components”.  41  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cThe next segment will introduce a set of products that allow developers to perform   governance of the data, model and code assets. We will discuss products like Google   Cloud Dataplex, which centralizes the governance of model and data, Vertex ML Metadata   and Vertex Experiment, which allows developers to register experiments, their metrics   and artifacts.  The role of an AI platform for gen   AI operations  Alongside the explosion of both predictive and gen AI applications, AI platforms, like Vertex   AI,11 have emerged as indispensable tools for organizations seeking to leverage the power of   Artificial Intelligence (AI). These comprehensive platforms provide a unified environment that   streamlines the entire AI lifecycle, from data preparation and model training to deployment,   automation, continuous integration/continuous delivery (CI/CD), governance, and monitoring.  At the heart of an AI platform lies its ability to support diverse AI development needs.   Whether you seek to utilize pre-trained AI solutions, adapt existing models through tuning   or transfer learning, or embark on training your own large models, AI platforms provide the   infrastructure and tools necessary to support these journeys. The advent of these platforms   has revolutionized the way organizations approach AI, enabling them to productionize AI   applications in a secure, enterprise-ready, responsible, controlled and scalable manner.   These platforms'),\n",
              " Document(metadata={'retrived_from': 2, 'source': '/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf'}, page_content='AI using ML OpsSeptember 2024\\x0cChain & Augment: Vertex AI Grounding, Extensions, and RAG  building blocks  Beyond training, tuning and adapting models and prompts directly, Vertex AI offers a   comprehensive ecosystem for augmenting LLMs, to address the challenges of factual   grounding and hallucination. The platform incorporates emerging techniques like RAG and   agent-based approaches.  RAG overcomes limitations by enriching prompts with data retrieved from vector databases,   circumventing pre-training requirements and ensuring the integration of up-to-date   information. Agent-based approaches, popularized by ReAct prompting, leverage LLMs as   mediators interacting with tools like RAG systems, APIs, and custom extensions. Vertex AI   facilitates this dynamic information source selection, enabling complex queries, real-time   actions, and the creation of multi-agent systems connected to vast information networks for   sophisticated query processing and real-time decision-making.  Vertex AI function calling69 empowers users by enhancing the capabilities of language  models (LLMs). It enables LLMs to access real-time data and interact with external systems,   providing users with more accurate and up-to-date information. To do that, users need to   provide function definitions such as description, inputs, outputs to the gen AI model. Instead   of directly executing functions, the LLM intelligently analyzes user requests and generates   structured data outputs. These outputs propose which function to call and what arguments   to use.  Vertex AI Grounding5 helps users connect large models with verifiable information by  grounding them to internal data corpora on Vertex AI Agent Builder70 or external sources   using Google Search. This enables two key functionalities: verifying model-generated outputs   against internal or external sources and creating RAG systems using Google’s advanced   search capabilities that produce quality content grounded in your own or web search data.   52  Operationalizing Generative'),\n",
              " Document(metadata={'retrived_from': 1, 'source': '/content/Newwhitepaper_Agents.pdf'}, page_content=\"a model can   leverage a database retrieval tool to access specific information, like a customer's purchase   history, so it can generate tailored shopping recommendations. Alternatively, based on a   user's query, a model can make various API calls to send an email response to a colleague   or complete a financial transaction on your behalf. To do so, the model must not only have   access to a set of external tools, it needs the ability to plan and execute any task in a self-  directed fashion. This combination of reasoning, logic, and access to external information   that are all connected to a Generative AI model invokes the concept of an agent, or a   program that extends beyond the standalone capabilities of a Generative AI model. This   whitepaper dives into all these and associated aspects in more detail.  4  AgentsSeptember 2024\\x0cWhat is an agent?  In its most fundamental form, a Generative AI agent can be defined as an application that   attempts to achieve a goal by observing the world and acting upon it using the tools that it   has at its disposal. Agents are autonomous and can act independently of human intervention,   especially when provided with proper goals or objectives they are meant to achieve. Agents   can also be proactive in their approach to reaching their goals. Even in the absence of   explicit instruction sets from a human, an agent can reason about what it should do next to   achieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this   whitepaper focuses on the specific types of agents that Generative AI models are capable of   building at the time of publication.  In order to understand the inner workings of an agent, let’s first introduce the foundational   components that drive the agent’s behavior, actions, and decision making. The combination   of these components can be described as a cognitive architecture, and there are many   such architectures that can be achieved by the mixing and matching of these components.   Focusing on the core\"),\n",
              " Document(metadata={'retrived_from': 1, 'source': '/content/Newwhitepaper_Agents.pdf'}, page_content=\"Agents  Authors: Julia Wiesinger, Patrick Marlow   and Vladimir Vuskovic  \\x0cAcknowledgements  Reviewers and Contributors  Evan Huang  Emily Xue  Olcan Sercinoglu  Sebastian Riedel  Satinder Baveja  Antonio Gulli  Anant Nawalgaria  Curators and Editors  Antonio Gulli  Anant Nawalgaria  Grace Mollison   Technical Writer  Joey Haymaker  Designer  Michael Lanning   2  AgentsSeptember 2024\\x0cTable of contents  Introduction   What is an agent?   The model   The tools   The orchestration layer   Agents vs. models     Cognitive architectures: How agents operate    Tools: Our keys to the outside world   Extensions    Sample Extensions    Functions    Use cases   Function sample code     Data stores   Implementation and application   Tools recap   Enhancing model performance with targeted learning   Agent quick start with LangChain   Production applications with Vertex AI agents   Summary   Endnotes   4  5  6  7  7  8  8  12  13  15  18  21  24  27  28  32  33  35  38  40  42                                \\x0cThis combination of reasoning,  logic, and access to external  information that are all connected  to a Generative AI model invokes  the concept of an agent.  Introduction  Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools   - like books, Google Search, or a calculator - to supplement their prior knowledge before   arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools   to access real-time information or suggest a real-world action. For example, a model can   leverage a database retrieval tool to access specific information, like a customer's purchase   history, so it can generate tailored shopping recommendations. Alternatively, based on a   user's query, a model can make various API calls to send an email response to a colleague   or complete a financial transaction on your behalf. To do so, the model must not only have   access to a set of external tools, it needs the ability to plan and execute any task in a self-  directed fashion. This\")]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract and print only the page content from each document\n",
        "import pandas as pd\n",
        "\n",
        "retrieval_df = pd.DataFrame()\n",
        "\n",
        "page_content = []\n",
        "retrieval_source = []\n",
        "pdf_source = []\n",
        "\n",
        "for doc in docs:\n",
        "    page_content.append(doc.page_content)\n",
        "    retrieval_source.append(doc.metadata['retrived_from'])\n",
        "    pdf_source.append(doc.metadata['source'])\n",
        "\n",
        "retrieval_df['page_content'] = page_content\n",
        "retrieval_df['retrieval_source'] = retrieval_source\n",
        "retrieval_df['pdf_source'] = pdf_source\n",
        "\n",
        "retrieval_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "C8ypvcYTkVOm",
        "outputId": "ec26cb38-ac08-4d9f-d389-13d404ff30bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        page_content  retrieval_source  \\\n",
              "0  AI Agent Builder, you are be able to easily gr...                 2   \n",
              "1  what arguments   to use.  Vertex AI Grounding5...                 2   \n",
              "2  is required. Depending on the use case, levera...                 2   \n",
              "3  and models used and   their lineage, the code ...                 2   \n",
              "4  AI using ML OpsSeptember 2024\n",
              "Chain & Augment:...                 2   \n",
              "5  a model can   leverage a database retrieval to...                 1   \n",
              "6  Agents  Authors: Julia Wiesinger, Patrick Marl...                 1   \n",
              "\n",
              "                                          pdf_source  \n",
              "0  /content/Newwhitepaper_Operationalizing Genera...  \n",
              "1  /content/Newwhitepaper_Operationalizing Genera...  \n",
              "2  /content/Newwhitepaper_Operationalizing Genera...  \n",
              "3  /content/Newwhitepaper_Operationalizing Genera...  \n",
              "4  /content/Newwhitepaper_Operationalizing Genera...  \n",
              "5                  /content/Newwhitepaper_Agents.pdf  \n",
              "6                  /content/Newwhitepaper_Agents.pdf  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-691dc56d-01b3-4060-85fd-64a55ecc665b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>page_content</th>\n",
              "      <th>retrieval_source</th>\n",
              "      <th>pdf_source</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AI Agent Builder, you are be able to easily gr...</td>\n",
              "      <td>2</td>\n",
              "      <td>/content/Newwhitepaper_Operationalizing Genera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>what arguments   to use.  Vertex AI Grounding5...</td>\n",
              "      <td>2</td>\n",
              "      <td>/content/Newwhitepaper_Operationalizing Genera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>is required. Depending on the use case, levera...</td>\n",
              "      <td>2</td>\n",
              "      <td>/content/Newwhitepaper_Operationalizing Genera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>and models used and   their lineage, the code ...</td>\n",
              "      <td>2</td>\n",
              "      <td>/content/Newwhitepaper_Operationalizing Genera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AI using ML OpsSeptember 2024\fChain &amp; Augment:...</td>\n",
              "      <td>2</td>\n",
              "      <td>/content/Newwhitepaper_Operationalizing Genera...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>a model can   leverage a database retrieval to...</td>\n",
              "      <td>1</td>\n",
              "      <td>/content/Newwhitepaper_Agents.pdf</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Agents  Authors: Julia Wiesinger, Patrick Marl...</td>\n",
              "      <td>1</td>\n",
              "      <td>/content/Newwhitepaper_Agents.pdf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-691dc56d-01b3-4060-85fd-64a55ecc665b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-691dc56d-01b3-4060-85fd-64a55ecc665b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-691dc56d-01b3-4060-85fd-64a55ecc665b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fcaec577-7bef-4c9c-8099-6b7d01c31014\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fcaec577-7bef-4c9c-8099-6b7d01c31014')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fcaec577-7bef-4c9c-8099-6b7d01c31014 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "retrieval_df",
              "summary": "{\n  \"name\": \"retrieval_df\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"page_content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"AI Agent Builder, you are be able to easily ground your agents by pointing to a diverse range   of data sources, including structured datastores such us BigQuery, Spanner, Cloud SQL,   unstructured sources like website content crawling and cloud storage as well as connectors   to Google drive and other APIs. Agent Builder utilizes a robust foundation of Google Search   technologies, encompassing semantic search, content chunking, ranking, algorithms,   and user intent understanding. Under the hood it optimizes document loading, chunking,   embedding models, and ranking strategies. It abstracts away these complexities and allows   users to simply specify their data source to initiate the gen AI-powered agent.This approach   is ideal for organizations seeking to build robust search experiences for standard use cases   without extensive technical expertise.   Vector databases are specialized systems for managing multi-dimensional data. This data,   encompassing images, text, audio, video, and other structured or unstructured formats,   is represented as vectors capturing its semantic meaning. Vector databases accelerate   searching and retrieval within these high-dimensional spaces, enabling efficient tasks like   finding similar images from billions or extracting relevant text snippets based on various   53  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\finputs. For a deeper dive into these topics, refer to 4 and 19. Vertex AI offers three flexible   solutions for storing and serving embeddings at scale, catering to diverse use cases and   user profiles.  Vertex AI Vector Search7 is a highly scalable low-latency similarity search and fully  managed vector database scaling to billions of vector embeddings with auto-scaling. This   technology, built upon ScaNN72 (a Google-developed technology used in products like   Search, YouTube, and Play), allows you to search from billions of semantically similar or   related items within your stored data. In the context of gen AI, the most common use\",\n          \"what arguments   to use.  Vertex AI Grounding5 helps users connect large models with verifiable information by  grounding them to internal data corpora on Vertex AI Agent Builder70 or external sources   using Google Search. This enables two key functionalities: verifying model-generated outputs   against internal or external sources and creating RAG systems using Google\\u2019s advanced   search capabilities that produce quality content grounded in your own or web search data.   52  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\fVertex AI extensions6 let developers integrate Vertex Foundation Models with real-time  data and real-world actions through APIs and functions, enabling task execution and allowing   enhanced capabilities. This extends to leveraging 1st party extensions like Vertex AI Search7   and Code Interpreter,71 or 3rd party extensions for triggering and completing transactions.   Imagine building an application that leverages the LLM's knowledge to plan a trip and   seamlessly utilizes internal APIs to book hotels and flights, all within a single interface.   Additionally, Vertex Extensions facilitate function calling with the gemini-pro model, enabling   you to generate descriptions, pass them to the large model, receive JSON with function   arguments, and automatically call the function.  Vertex AI Agent Builder70 is an out-of-the-box solution that allows you to quickly build gen  AI agents, to be used as conversational chatbots or as part of a search engine. With Vertex   AI Agent Builder, you are be able to easily ground your agents by pointing to a diverse range   of data sources, including structured datastores such us BigQuery, Spanner, Cloud SQL,   unstructured sources like website content crawling and cloud storage as well as connectors   to Google drive and other APIs. Agent Builder utilizes a robust foundation of Google Search   technologies, encompassing semantic search, content chunking, ranking, algorithms,   and user intent understanding. Under the hood it\",\n          \"a model can   leverage a database retrieval tool to access specific information, like a customer's purchase   history, so it can generate tailored shopping recommendations. Alternatively, based on a   user's query, a model can make various API calls to send an email response to a colleague   or complete a financial transaction on your behalf. To do so, the model must not only have   access to a set of external tools, it needs the ability to plan and execute any task in a self-  directed fashion. This combination of reasoning, logic, and access to external information   that are all connected to a Generative AI model invokes the concept of an agent, or a   program that extends beyond the standalone capabilities of a Generative AI model. This   whitepaper dives into all these and associated aspects in more detail.  4  AgentsSeptember 2024\\fWhat is an agent?  In its most fundamental form, a Generative AI agent can be defined as an application that   attempts to achieve a goal by observing the world and acting upon it using the tools that it   has at its disposal. Agents are autonomous and can act independently of human intervention,   especially when provided with proper goals or objectives they are meant to achieve. Agents   can also be proactive in their approach to reaching their goals. Even in the absence of   explicit instruction sets from a human, an agent can reason about what it should do next to   achieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this   whitepaper focuses on the specific types of agents that Generative AI models are capable of   building at the time of publication.  In order to understand the inner workings of an agent, let\\u2019s first introduce the foundational   components that drive the agent\\u2019s behavior, actions, and decision making. The combination   of these components can be described as a cognitive architecture, and there are many   such architectures that can be achieved by the mixing and matching of these components.   Focusing on the core\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieval_source\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdf_source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"/content/Newwhitepaper_Agents.pdf\",\n          \"/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_df.shape"
      ],
      "metadata": {
        "id": "53Db00rlnWGu",
        "outputId": "c1d8417a-2a4e-4e6f-f28c-81aef7824546",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re Ranking"
      ],
      "metadata": {
        "id": "JP3qZGNzoMnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory where the Chroma database will persist data\n",
        "persist_directory = \"/content/re-rank\"\n",
        "\n",
        "# Initialize Cohere embeddings with the specified model\n",
        "# \"embed-english-v3.0\" is a pre-trained English language embedding model by Cohere\n",
        "# The user_agent parameter specifies the tool or library using the Cohere API, in this case, LangChain\n",
        "embedding = CohereEmbeddings(\n",
        "    model=\"embed-english-v3.0\",\n",
        "    user_agent=\"langchain\"\n",
        ")"
      ],
      "metadata": {
        "id": "-927YuLknYLN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_to_vectordb(file_path,source):\n",
        "  # Loop through a list of PDF files to process\n",
        "\n",
        "  for pdf_name in [file_path]:\n",
        "      # Open each PDF file in binary mode\n",
        "      with open(pdf_name, 'rb') as f:\n",
        "          # Extract text from the PDF using the extract_text_pdf_miner function\n",
        "          text = extract_text_pdf_miner(f)\n",
        "\n",
        "          # Clean the extracted text by removing newline characters and joining into a single string\n",
        "          cleaned_text = \" \".join(text.split(\"\\n\"))\n",
        "\n",
        "          # Initialize a list to store document chunks\n",
        "          docs = []\n",
        "\n",
        "          # Create a text splitter to divide the text into manageable chunks\n",
        "          # Each chunk has a maximum size of 2048 characters with a 512-character overlap\n",
        "          splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
        "\n",
        "          # Split the cleaned text into chunks and wrap each chunk in a Document object\n",
        "          for chunk in splitter.split_text(cleaned_text):\n",
        "              docs.append(Document(page_content=chunk, metadata={\"source\": pdf_name}))\n",
        "      # Create a Chroma collection from the processed documents\n",
        "      # Use the specified persist directory and embedding model for storage and retrieval\n",
        "\n",
        "      db = Chroma.from_documents(\n",
        "            documents=docs,\n",
        "            persist_directory=persist_directory,\n",
        "            embedding=embedding\n",
        "        )\n",
        "\n",
        "load_data_to_vectordb(file_path=\"/content/Newwhitepaper_Agents.pdf\",source=1)\n",
        "load_data_to_vectordb(file_path=\"/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf\",source=2)"
      ],
      "metadata": {
        "id": "odGWYjzEoUCA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank"
      ],
      "metadata": {
        "id": "j0RFeoW6o6bf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsearch = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
      ],
      "metadata": {
        "id": "8Shje5B2o1tQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty DataFrame with specified columns\n",
        "non_rerank_df = pd.DataFrame(columns=['Text', 'source', 'relevance_score'])\n",
        "\n",
        "# Perform similarity search using a preconfigured document search tool\n",
        "# This retrieves the top 3 documents based on relevance to the query\n",
        "res_docs = docsearch.similarity_search_with_relevance_scores(\"What is the AI agents?\", k=3)\n",
        "\n",
        "# Loop through the retrieved documents and populate the DataFrame\n",
        "for doc in res_docs:\n",
        "    non_rerank_df = non_rerank_df._append(\n",
        "        {\n",
        "            'Text': doc[0].page_content,  # Extract the page content (text) from the document\n",
        "            'source': doc[0].metadata['source'],  # Extract the source metadata\n",
        "            'relevance_score': doc[1]  # Extract the relevance score\n",
        "        },\n",
        "        ignore_index=True  # Ensure the new row is appended without overwriting existing rows\n",
        "    )\n",
        "\n",
        "# Display the first 3 rows of the DataFrame\n",
        "non_rerank_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "xYXIYoBLqNmZ",
        "outputId": "b48831c8-ce5f-4e54-f5bc-42873a5991dd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-79a2022a2232>:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  non_rerank_df = non_rerank_df._append(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  \\\n",
              "0  Agents  Authors: Julia Wiesinger, Patrick Marl...   \n",
              "1  a model can   leverage a database retrieval to...   \n",
              "2  of language models by leveraging tools to acce...   \n",
              "\n",
              "                              source  relevance_score  \n",
              "0  /content/Newwhitepaper_Agents.pdf         0.360925  \n",
              "1  /content/Newwhitepaper_Agents.pdf         0.357930  \n",
              "2  /content/Newwhitepaper_Agents.pdf         0.342624  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c8addff-2174-471e-80a5-96fd9055d308\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>source</th>\n",
              "      <th>relevance_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Agents  Authors: Julia Wiesinger, Patrick Marl...</td>\n",
              "      <td>/content/Newwhitepaper_Agents.pdf</td>\n",
              "      <td>0.360925</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a model can   leverage a database retrieval to...</td>\n",
              "      <td>/content/Newwhitepaper_Agents.pdf</td>\n",
              "      <td>0.357930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>of language models by leveraging tools to acce...</td>\n",
              "      <td>/content/Newwhitepaper_Agents.pdf</td>\n",
              "      <td>0.342624</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c8addff-2174-471e-80a5-96fd9055d308')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5c8addff-2174-471e-80a5-96fd9055d308 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5c8addff-2174-471e-80a5-96fd9055d308');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a0b7b6ca-04da-47c1-a311-fb3202629f7d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a0b7b6ca-04da-47c1-a311-fb3202629f7d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a0b7b6ca-04da-47c1-a311-fb3202629f7d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "non_rerank_df",
              "summary": "{\n  \"name\": \"non_rerank_df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Agents  Authors: Julia Wiesinger, Patrick Marlow   and Vladimir Vuskovic  \\fAcknowledgements  Reviewers and Contributors  Evan Huang  Emily Xue  Olcan Sercinoglu  Sebastian Riedel  Satinder Baveja  Antonio Gulli  Anant Nawalgaria  Curators and Editors  Antonio Gulli  Anant Nawalgaria  Grace Mollison   Technical Writer  Joey Haymaker  Designer  Michael Lanning   2  AgentsSeptember 2024\\fTable of contents  Introduction   What is an agent?   The model   The tools   The orchestration layer   Agents vs. models     Cognitive architectures: How agents operate    Tools: Our keys to the outside world   Extensions    Sample Extensions    Functions    Use cases   Function sample code     Data stores   Implementation and application   Tools recap   Enhancing model performance with targeted learning   Agent quick start with LangChain   Production applications with Vertex AI agents   Summary   Endnotes   4  5  6  7  7  8  8  12  13  15  18  21  24  27  28  32  33  35  38  40  42                                \\fThis combination of reasoning,  logic, and access to external  information that are all connected  to a Generative AI model invokes  the concept of an agent.  Introduction  Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools   - like books, Google Search, or a calculator - to supplement their prior knowledge before   arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools   to access real-time information or suggest a real-world action. For example, a model can   leverage a database retrieval tool to access specific information, like a customer's purchase   history, so it can generate tailored shopping recommendations. Alternatively, based on a   user's query, a model can make various API calls to send an email response to a colleague   or complete a financial transaction on your behalf. To do so, the model must not only have   access to a set of external tools, it needs the ability to plan and execute any task in a self-  directed fashion. This\",\n          \"a model can   leverage a database retrieval tool to access specific information, like a customer's purchase   history, so it can generate tailored shopping recommendations. Alternatively, based on a   user's query, a model can make various API calls to send an email response to a colleague   or complete a financial transaction on your behalf. To do so, the model must not only have   access to a set of external tools, it needs the ability to plan and execute any task in a self-  directed fashion. This combination of reasoning, logic, and access to external information   that are all connected to a Generative AI model invokes the concept of an agent, or a   program that extends beyond the standalone capabilities of a Generative AI model. This   whitepaper dives into all these and associated aspects in more detail.  4  AgentsSeptember 2024\\fWhat is an agent?  In its most fundamental form, a Generative AI agent can be defined as an application that   attempts to achieve a goal by observing the world and acting upon it using the tools that it   has at its disposal. Agents are autonomous and can act independently of human intervention,   especially when provided with proper goals or objectives they are meant to achieve. Agents   can also be proactive in their approach to reaching their goals. Even in the absence of   explicit instruction sets from a human, an agent can reason about what it should do next to   achieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this   whitepaper focuses on the specific types of agents that Generative AI models are capable of   building at the time of publication.  In order to understand the inner workings of an agent, let\\u2019s first introduce the foundational   components that drive the agent\\u2019s behavior, actions, and decision making. The combination   of these components can be described as a cognitive architecture, and there are many   such architectures that can be achieved by the mixing and matching of these components.   Focusing on the core\",\n          \"of language models by leveraging tools to access real-  time information, suggest real-world actions, and plan and execute complex tasks   autonomously. agents can leverage one or more language models to decide when and   how to transition through states and use external tools to complete any number of   complex tasks that would be difficult or impossible for the model to complete on its own.  2.  At the heart of an agent\\u2019s operation is the orchestration layer, a cognitive architecture that   structures reasoning, planning, decision-making and guides its actions. Various reasoning   techniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework   for the orchestration layer to take in information, perform internal reasoning, and generate   informed decisions or responses.   3.  Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside   world for agents, allowing them to interact with external systems and access knowledge   beyond their training data. Extensions provide a bridge between agents and external APIs,   enabling the execution of API calls and retrieval of real-time information. functions provide   a more nuanced control for the developer through the division of labor, allowing agents   to generate Function parameters which can be executed client-side. Data Stores provide   agents with access to structured or unstructured data, enabling data-driven applications.  The future of agents holds exciting advancements and we\\u2019ve only begun to scratch the   surface of what is possible. As tools become more sophisticated and reasoning capabilities   are enhanced, agents will be empowered to solve increasingly complex problems.   Furthermore, the strategic approach of \\u2018agent chaining\\u2019 will continue to gain momentum. By   40  AgentsSeptember 2024\\fcombining specialized agents - each excelling in a particular domain or task - we can create   a \\u2018mixture of agent experts\\u2019 approach, capable of delivering exceptional results across   various industries and problem\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"/content/Newwhitepaper_Agents.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00981632635175125,\n        \"min\": 0.34262413983279083,\n        \"max\": 0.3609251022463428,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.3609251022463428\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and initialize the reranker for document compression\n",
        "compressor = CohereRerank()\n",
        "# CohereRerank is a model or tool designed to rerank documents based on their relevance.\n",
        "\n",
        "# Create a ContextualCompressionRetriever for improved document retrieval\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,               # Use the reranker as the base compression mechanism\n",
        "    base_retriever=docsearch.as_retriever()   # Use the existing document search tool as the base retriever\n",
        ")\n",
        "# The ContextualCompressionRetriever combines the base retriever's results with reranking\n",
        "# to provide more contextually relevant and concise results."
      ],
      "metadata": {
        "id": "LooPA8hZo8mX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c01c04-8421-4113-d735-e13ef4957229"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-f983c412f7af>:2: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
            "  compressor = CohereRerank()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty DataFrame with specified columns\n",
        "source_df = pd.DataFrame(columns=['Text', 'source', 'relevance_score'])\n",
        "\n",
        "# Retrieve compressed documents relevant to the query using the contextual compression retriever\n",
        "compressed_docs = compression_retriever.get_relevant_documents(\"What is the AI agents and how does that work?\")\n",
        "\n",
        "# Loop through the first 3 compressed documents and populate the DataFrame\n",
        "for i in range(3):\n",
        "    source_df = source_df._append(\n",
        "        {\n",
        "            'Text': compressed_docs[i].page_content,  # Extract the content of the document\n",
        "            'source': compressed_docs[i].metadata['source'],  # Extract the source information\n",
        "            'relevance_score': compressed_docs[i].metadata['relevance_score']  # Extract the relevance score\n",
        "        },\n",
        "        ignore_index=True  # Ensure the new row is appended without overwriting existing rows\n",
        "    )\n",
        "\n",
        "# Display the first 3 rows of the DataFrame\n",
        "source_df.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "ZB2e0L15pCc3",
        "outputId": "39733931-19d6-4496-b16b-0e5c05c4913c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-911209b96348>:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  source_df = source_df._append(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Text  \\\n",
              "0  a model can   leverage a database retrieval to...   \n",
              "1  Agents  Authors: Julia Wiesinger, Patrick Marl...   \n",
              "2  of language models by leveraging tools to acce...   \n",
              "\n",
              "                              source  relevance_score  \n",
              "0  /content/Newwhitepaper_Agents.pdf         0.991938  \n",
              "1  /content/Newwhitepaper_Agents.pdf         0.940790  \n",
              "2  /content/Newwhitepaper_Agents.pdf         0.919787  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1814cc40-4597-4f93-b650-9f97b9b8cb4f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>source</th>\n",
              "      <th>relevance_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a model can   leverage a database retrieval to...</td>\n",
              "      <td>/content/Newwhitepaper_Agents.pdf</td>\n",
              "      <td>0.991938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Agents  Authors: Julia Wiesinger, Patrick Marl...</td>\n",
              "      <td>/content/Newwhitepaper_Agents.pdf</td>\n",
              "      <td>0.940790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>of language models by leveraging tools to acce...</td>\n",
              "      <td>/content/Newwhitepaper_Agents.pdf</td>\n",
              "      <td>0.919787</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1814cc40-4597-4f93-b650-9f97b9b8cb4f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1814cc40-4597-4f93-b650-9f97b9b8cb4f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1814cc40-4597-4f93-b650-9f97b9b8cb4f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a6db357f-3347-4aee-9b35-2fc26883adf4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a6db357f-3347-4aee-9b35-2fc26883adf4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a6db357f-3347-4aee-9b35-2fc26883adf4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "source_df",
              "summary": "{\n  \"name\": \"source_df\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"a model can   leverage a database retrieval tool to access specific information, like a customer's purchase   history, so it can generate tailored shopping recommendations. Alternatively, based on a   user's query, a model can make various API calls to send an email response to a colleague   or complete a financial transaction on your behalf. To do so, the model must not only have   access to a set of external tools, it needs the ability to plan and execute any task in a self-  directed fashion. This combination of reasoning, logic, and access to external information   that are all connected to a Generative AI model invokes the concept of an agent, or a   program that extends beyond the standalone capabilities of a Generative AI model. This   whitepaper dives into all these and associated aspects in more detail.  4  AgentsSeptember 2024\\fWhat is an agent?  In its most fundamental form, a Generative AI agent can be defined as an application that   attempts to achieve a goal by observing the world and acting upon it using the tools that it   has at its disposal. Agents are autonomous and can act independently of human intervention,   especially when provided with proper goals or objectives they are meant to achieve. Agents   can also be proactive in their approach to reaching their goals. Even in the absence of   explicit instruction sets from a human, an agent can reason about what it should do next to   achieve its ultimate goal. While the notion of agents in AI is quite general and powerful, this   whitepaper focuses on the specific types of agents that Generative AI models are capable of   building at the time of publication.  In order to understand the inner workings of an agent, let\\u2019s first introduce the foundational   components that drive the agent\\u2019s behavior, actions, and decision making. The combination   of these components can be described as a cognitive architecture, and there are many   such architectures that can be achieved by the mixing and matching of these components.   Focusing on the core\",\n          \"Agents  Authors: Julia Wiesinger, Patrick Marlow   and Vladimir Vuskovic  \\fAcknowledgements  Reviewers and Contributors  Evan Huang  Emily Xue  Olcan Sercinoglu  Sebastian Riedel  Satinder Baveja  Antonio Gulli  Anant Nawalgaria  Curators and Editors  Antonio Gulli  Anant Nawalgaria  Grace Mollison   Technical Writer  Joey Haymaker  Designer  Michael Lanning   2  AgentsSeptember 2024\\fTable of contents  Introduction   What is an agent?   The model   The tools   The orchestration layer   Agents vs. models     Cognitive architectures: How agents operate    Tools: Our keys to the outside world   Extensions    Sample Extensions    Functions    Use cases   Function sample code     Data stores   Implementation and application   Tools recap   Enhancing model performance with targeted learning   Agent quick start with LangChain   Production applications with Vertex AI agents   Summary   Endnotes   4  5  6  7  7  8  8  12  13  15  18  21  24  27  28  32  33  35  38  40  42                                \\fThis combination of reasoning,  logic, and access to external  information that are all connected  to a Generative AI model invokes  the concept of an agent.  Introduction  Humans are fantastic at messy pattern recognition tasks. However, they often rely on tools   - like books, Google Search, or a calculator - to supplement their prior knowledge before   arriving at a conclusion. Just like humans, Generative AI models can be trained to use tools   to access real-time information or suggest a real-world action. For example, a model can   leverage a database retrieval tool to access specific information, like a customer's purchase   history, so it can generate tailored shopping recommendations. Alternatively, based on a   user's query, a model can make various API calls to send an email response to a colleague   or complete a financial transaction on your behalf. To do so, the model must not only have   access to a set of external tools, it needs the ability to plan and execute any task in a self-  directed fashion. This\",\n          \"of language models by leveraging tools to access real-  time information, suggest real-world actions, and plan and execute complex tasks   autonomously. agents can leverage one or more language models to decide when and   how to transition through states and use external tools to complete any number of   complex tasks that would be difficult or impossible for the model to complete on its own.  2.  At the heart of an agent\\u2019s operation is the orchestration layer, a cognitive architecture that   structures reasoning, planning, decision-making and guides its actions. Various reasoning   techniques such as ReAct, Chain-of-Thought, and Tree-of-Thoughts, provide a framework   for the orchestration layer to take in information, perform internal reasoning, and generate   informed decisions or responses.   3.  Tools, such as Extensions, Functions, and Data Stores, serve as the keys to the outside   world for agents, allowing them to interact with external systems and access knowledge   beyond their training data. Extensions provide a bridge between agents and external APIs,   enabling the execution of API calls and retrieval of real-time information. functions provide   a more nuanced control for the developer through the division of labor, allowing agents   to generate Function parameters which can be executed client-side. Data Stores provide   agents with access to structured or unstructured data, enabling data-driven applications.  The future of agents holds exciting advancements and we\\u2019ve only begun to scratch the   surface of what is possible. As tools become more sophisticated and reasoning capabilities   are enhanced, agents will be empowered to solve increasingly complex problems.   Furthermore, the strategic approach of \\u2018agent chaining\\u2019 will continue to gain momentum. By   40  AgentsSeptember 2024\\fcombining specialized agents - each excelling in a particular domain or task - we can create   a \\u2018mixture of agent experts\\u2019 approach, capable of delivering exceptional results across   various industries and problem\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"/content/Newwhitepaper_Agents.pdf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relevance_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03711033230385712,\n        \"min\": 0.91978675,\n        \"max\": 0.99193794,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.99193794\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the last result is different and the relevance scores are also different but they are according to the relevance model"
      ],
      "metadata": {
        "id": "iaYcHLO3suLs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_2TS6houIj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}