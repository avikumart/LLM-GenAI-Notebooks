{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b202d0dacb7842a3bc956b6593ecf8fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d365c753ff84dfea8a1263a81c0cae3",
              "IPY_MODEL_5dd557e80eb642d7905889c03fce4b54",
              "IPY_MODEL_6dda76a90efb47edbb2ffa5c4a7b4602"
            ],
            "layout": "IPY_MODEL_160fadcdb18c49e3916e590fd20ed8b6"
          }
        },
        "3d365c753ff84dfea8a1263a81c0cae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adf9802be90f49e29d758eb8e69dffd1",
            "placeholder": "​",
            "style": "IPY_MODEL_f8b6ee208da6407192e0190b0a6d6dbe",
            "value": "Evaluating: 100%"
          }
        },
        "5dd557e80eb642d7905889c03fce4b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a864ecdfd6854a11bec5599ac16097da",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e290443588d43f68de62bf003108415",
            "value": 6
          }
        },
        "6dda76a90efb47edbb2ffa5c4a7b4602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c33216b44d7542b6a806d29f4ff58a21",
            "placeholder": "​",
            "style": "IPY_MODEL_5e0c3622b837482680e64ffda3615739",
            "value": " 6/6 [00:09&lt;00:00,  1.16s/it]"
          }
        },
        "160fadcdb18c49e3916e590fd20ed8b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adf9802be90f49e29d758eb8e69dffd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b6ee208da6407192e0190b0a6d6dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a864ecdfd6854a11bec5599ac16097da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e290443588d43f68de62bf003108415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c33216b44d7542b6a806d29f4ff58a21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e0c3622b837482680e64ffda3615739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/TMLC_LLM_projects/RAG/Evaluating_RAG_pipeline_with_RAGAs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the same notebook as in Personal resource assistant. For explanation of code till Evaluation kindly check out that notebook first."
      ],
      "metadata": {
        "id": "WTKVZfnom-SH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing necessary libraries"
      ],
      "metadata": {
        "id": "8UblPEH_V5FM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8HmgD6eLRrzA",
        "outputId": "e9d129b4-6f60-4639-916a-c7ec2e41f416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain-cohere langchain pdfminer.six chromadb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "FAFkZ2UdV71s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"COHERE_API_KEY\"] = userdata.get('COHERE_KEY')\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_cohere import ChatCohere\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from pdfminer.high_level import extract_text as extract_text_pdf_miner\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import CohereEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain_core.runnables import RunnableParallel,RunnablePassthrough"
      ],
      "metadata": {
        "id": "RpRhM8WhR2Vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b42af9ad-e54b-43ab-d98d-460984495c41"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3553: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## VectorDB setup"
      ],
      "metadata": {
        "id": "DnFFjExUWAzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = \"/content/chroma_db\"\n",
        "\n",
        "embedding = CohereEmbeddings(\n",
        "    model=\"embed-english-v3.0\",\n",
        "    user_agent=\"langchain\"\n",
        ")\n",
        "\n",
        "for pdf_name in [\"/content/Newwhitepaper_Agents.pdf\", \"/content/Newwhitepaper_Operationalizing Generative AI on Vertex AI.pdf\"]:\n",
        "    with open(pdf_name, 'rb') as f:\n",
        "        text = extract_text_pdf_miner(f)\n",
        "        cleaned_text = \" \".join(text.split(\"\\n\"))\n",
        "        docs = []\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=512)\n",
        "        for chunk in splitter.split_text(cleaned_text):\n",
        "            docs.append(Document(page_content=chunk, metadata={\"source\": pdf_name}))\n",
        "    vector_collection_fixed_size = Chroma.from_documents(\n",
        "        documents=docs,\n",
        "        persist_directory=persist_directory,\n",
        "        embedding=embedding\n",
        "    )\n",
        "\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)"
      ],
      "metadata": {
        "id": "92kuclutTc9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6843c1b-f111-4f2f-a9c2-8d3be37599e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-880a7a0b4320>:3: LangChainDeprecationWarning: The class `CohereEmbeddings` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereEmbeddings``.\n",
            "  embedding = CohereEmbeddings(\n",
            "<ipython-input-3-880a7a0b4320>:22: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
            "  vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG pipeline"
      ],
      "metadata": {
        "id": "LJ8aXhznniF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatCohere(model=\"command-r\")\n",
        "\n",
        "prompt_str = \"\"\"Answer the question below using the context:\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"context\": vectordb.as_retriever(),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = retrieval | prompt | llm | output_parser"
      ],
      "metadata": {
        "id": "7mUIP130UsTf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(\"What is AI agents?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydFLc8lbVG9V",
        "outputId": "99615082-5d8e-4d12-d1f2-d35072b11c39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI agents, or Generative AI agents, are applications that use a combination of reasoning, logic, and access to external information to achieve specific goals. They are designed to work autonomously and can act independently, leveraging various tools and cognitive architectures to observe the world, make decisions, and take action. These agents can access real-time information, suggest real-world actions, and execute complex tasks without human intervention. AI agents are particularly useful for messy pattern recognition tasks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "F15waKMWnfD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragas==0.2.0 langchain-openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lIr7qb63n9sN",
        "outputId": "9f86e4e8-f33d-4dac-903d-bed622cd3904"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_queries = [\n",
        "    \"What is the score of YOLO on VOC 2012 test set?\",\n",
        "    \"How many layers does the Encoder of Transformer is composed of?\",\n",
        "]\n",
        "\n",
        "# This are the one liner responses that humans might respond to these questions\n",
        "# While an LLM might produce a bigger response, our need if to identify that the response by LLM does not exclude the actual answer\n",
        "expected_responses = [\n",
        "    \"On the VOC 2012 test set, YOLO scores 57.9% mAP.\",\n",
        "    \"The encoder is composed of a stack of N = 6 identical layers\",\n",
        "]"
      ],
      "metadata": {
        "id": "hMBVqZ4foce1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = []\n",
        "\n",
        "for query,reference in zip(sample_queries,expected_responses):\n",
        "\n",
        "    relevant_docs = vectordb.similarity_search(query)\n",
        "    response = chain.invoke(query)\n",
        "    dataset.append(\n",
        "        {\n",
        "            \"user_input\":query,\n",
        "            \"retrieved_contexts\":[doc.page_content for doc in relevant_docs],\n",
        "            \"response\":response,\n",
        "            \"reference\":reference\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "M2VpVR8xobPX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-T9k3eGqVGV",
        "outputId": "31c4130b-bb2f-4eb2-aa93-98542df37904"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'user_input': 'What is the score of YOLO on VOC 2012 test set?',\n",
              "  'retrieved_contexts': ['Imagine the output   to a given input is factually inaccurate. How can you find out which of the components are   the ones that didn’t perform well? To answer this question it is necessary to apply logging   on the application level and at the component level. We need lineage in our logging for all   components executed. For every component we need to log their inputs and outputs. We   also need to be able to map those with any additional artifacts and parameters they depend   on so we can easily analyze those inputs and outputs.  Monitoring can be applied to the overall gen AI application and to individual components. We   prioritize monitoring at the application level. This is because if the application is performant   and monitoring proves that, it implies that all components are also performant. You can also   apply the same practices to each of the prompted model components to get more granular   results and understanding of your application.  Skew detection in traditional ML systems refers to training-serving skew that occurs when   the feature data distribution in production deviates from the feature data distribution   observed during model training. In the case of Gen AI systems using pretrained models in   components chained together to produce the output, we need to modify our approach. We   can measure skew by comparing the distribution of the input data we used to evaluate our   application (the test set as described under the Data Curation and Principles section above)   and the distribution of the inputs to our application in production. Once the two distributions   drift apart,further investigation is needed. The same process can be applied to the output   data as well.  38  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cFigure 11.  Drift/skew detection process overview  Like skew detection, the drift detection process checks for statistical differences between   two datasets. However, instead of comparing evaluations and serving inputs, drift looks for   changes in input',\n",
              "   'on specific tasks   and contexts within domain-specific datasets. The required examples resemble the one-  shot example structure employed in the construction of a prompt. This effectively extends   the few-shot learning approach for enhanced optimization. This focused tuning enables   the model to encode additional parameters in the model necessary for mimicking desired   behaviors such as improved complex prompt comprehension, adaptation to specific   output formats, correcting errors, and learning new tasks. The SFT tuning approach on   Vertex AI, minimizes computational overhead and time while yielding an updated model   that integrates the newly acquired parameters with the original model’s core parameters.  •  Reinforcement learning with human feedback (RLHF),64 available on Vertex AI for  foundational models like PaLM 2,and open-source models like T5 (s-xxl) and Llama2,   leverages human feedback to train large models to align with human preferences. This   technique is well-suited in complex tasks involving preference modeling and optimizes   LLMs on intricate, sequence-level objectives not easily addressed by traditional   supervised fine-tuning. The process involves first training a reward model using a human   preference dataset, then utilizing it to score the output from the LLM, and finally applying   reinforcement learning to optimize the LLM. This approach is recognized as a key driver of   success in conversational large language models.  •  Distillation step-by-step20 is an advanced distillation technique transferring knowledge  from a significantly larger model (known as teacher model) to a smaller task-specific   model (known as student model), preserving important information while reducing model   size. Step-by-Step Distillation20 surpasses common techniques by requiring significantly   less data. This method, accessible on Vertex AI,65 significantly reduces inference costs and   latencies while minimizing performance impact in the resulting smaller LLM.66  50  Operationalizing Generative AI',\n",
              "   \"inputs to our application in production. Once the two distributions   drift apart,further investigation is needed. The same process can be applied to the output   data as well.  38  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cFigure 11.  Drift/skew detection process overview  Like skew detection, the drift detection process checks for statistical differences between   two datasets. However, instead of comparing evaluations and serving inputs, drift looks for   changes in input data. This allows you to check how the inputs and therefore the behavior of   your users changed over time. This is the same as traditional MLOps.   Given that the input to the application is typically text, there are a few approaches to   measuring skew and drift. In general all the methods are trying to identify significant   changes in production data, both textual (size of input) and conceptual (topics in input),   when compared to the evaluation dataset. All these methods are looking for changes that   could potentially indicate the application might not be prepared to successfully handle the   nature of the new data that are now coming in. Some common approaches are calculating   embeddings and distances, counting text length and number of tokens, and tracking   vocabulary changes, new concepts and intents, prompts and topics in datasets, as well   as statistical approaches such as least-squares density difference,22 maximum mean   discrepancy (MMD),23 learned kernel MMD,24 or context-aware MMD.25 As gen AI use cases   are so diverse, it is often necessary to create additional custom metrics that better capture   abnormal changes in your data.  39  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cContinuous evaluation is another common approach to GenAI application monitoring. In   a continuous evaluation system, you capture the model's production output and run an   evaluation task using that output, to keep track of the model's performance over time. One   approach is collecting\",\n",
              "   \"AI systems, evaluation might have different degrees of automation: from   entirely driven by humans to entirely automated by a process.   In the early days of a project, when you're still prototyping, evaluation is often a manual   process. Developers eyeball the model's outputs, getting a qualitative sense of how it's   performing. But as the project matures and the number of test cases balloons, manual   evaluation becomes a bottleneck. That's when automation becomes key.  27  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cAutomating evaluation has two big benefits. First, it lets you move faster. Instead of spending   time manually checking each test case, you can let the machines do the heavy lifting.   This means more iterations, more experiments, and ultimately, a better product. Second,   automation makes evaluation more reliable. It takes human subjectivity out of the equation,   ensuring that results are reproducible.  But automating evaluation for gen AI comes with its own set of challenges.   For one, both the inputs (prompts) and outputs can be incredibly complex. A single prompt   might include multiple instructions and constraints that the model needs to juggle. And the   outputs themselves are often high-dimensional - think a generated image or a block of text.   Capturing the quality of these outputs in a simple metric is tough.  There are some established metrics, like BLEU for translations and ROUGE for summaries,   but they don't always tell the full story. That's where custom evaluation methods come in.   One approach is to use another foundational model as a judge. For example, you could   prompt a large language model to score the quality of generated texts across various   dimensions. This is the idea behind techniques like AutoSxS.16  Another challenge is the subjective nature of many evaluation metrics for gen AI. What   makes one output ‘better’ than another can often be a matter of opinion. The key here is to   make sure your automated evaluation aligns with\"],\n",
              "  'response': \"I'm sorry, I could not find any information about YOLO's score on the VOC 2012 test set in the provided context. The discussions in the given texts focus on generative AI operationalization, monitoring, and evaluation.\",\n",
              "  'reference': 'On the VOC 2012 test set, YOLO scores 57.9% mAP.'},\n",
              " {'user_input': 'How many layers does the Encoder of Transformer is composed of?',\n",
              "  'retrieved_contexts': ['to deploying the training and serving systems, to check both the compatibility of the model   with the defined serving configuration and the availability of the required hardware. There   are a number of optional infrastructure validation layers that can perform some of these   checks automatically. For instance, TFX19 has an infrastructure validation layer that checks   34  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cwhether the model will run correctly on a specified hardware configuration, which can help   catch configuration issues before deployment. Nevertheless, the availability of the required   hardware still needs to be verified by hand by the engineer or the system administrator.  Compression and optimization  Another way of addressing infrastructure challenges is to optimize the model itself.   Compressing and/or optimizing the model can often significantly reduce the storage and   compute resources needed for training and serving, and in many cases can also decrease   the serving latency.  Some techniques for model compression and optimization include quantization, distillation   and model pruning. Quantization reduces the size and computational requirements of the   model by converting its weights and activations from higher-precision floating-point numbers   to lower-precision representations, such as 8-bit integers or 16-bit floating-point numbers.   This can significantly reduce the memory footprint and computational overhead of the model.   Model Pruning is a technique for eliminating unnecessary weight parameters or by selecting   only important subnetworks within the model. This reduces model size while maintaining   accuracy as high as possible. Finally, distillation trains a smaller model, using the responses   generated by a larger LLM, to reproduce the output of the larger LLM for a specific domain.   This can significantly reduce the amount of training data, compute, and storage resources   needed for the application.  In certain situations, model distillation',\n",
              "   '42. Code models overview. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-  models-overview  65  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0c43. Convert speech to text. Available at: https://cloud.google.com/vertex-ai/docs/generative-ai/speech/  speech-to-text  44. Text-to-Speech AI. Available at: https://cloud.google.com/text-to-speech  45. Natural Language AI. Available at: https://cloud.google.com/natural-language  46. Translate docs, audio, and videos in real time with Google AI. Available at: https://cloud.google.com/  translate  47.  Vision AI. Available at: https://cloud.google.com/vision  48. Git. Available at: https://git-scm.com/  49. CodeGemma model card. Available at: https://ai.google.dev/gemma/docs/codegemma/model_card  50. TII’s Falcon. Available at: https://falconllm.tii.ae/  51.  Mistral AI. Available at: https://mistral.ai/  52. Hugging Face, 2024. Vision Transformer (ViT) Documentation. Hugging Face, [online] Available at:    https://huggingface.co/docs/transformers/en/model_doc/vit  53. Mingxing Tan, Quoc V. Le, 2019. EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.   Available at: https://arxiv.org/abs/1905.11946  54. Anthropic Claude 3. Available at: https://www.anthropic.com/news/claude-3-haiku  55. Anthropic Claude 3 on Google Cloud Model Garden. Available at: https://cloud.google.com/blog/products/  ai-machine-learning/announcing-anthropics-claude-3-models-in-google-cloud-vertex-ai  56. Vertex AI API. Available at: https://cloud.google.com/vertex-ai/docs/reference/rest  57.  Vertex AI: Python SDK. Available at: https://cloud.google.com/python/docs/reference/aiplatform/latest/ vertexai  58. Vertex AI: Node.js Client. Available at: https://cloud.google.com/nodejs/docs/reference/aiplatform/latest/  overview  59. Vertex AI for Java. Available at: https://cloud.google.com/java/docs/reference/google-cloud-aiplatform/  latest/overview  60. Customize and deploy generative models. Available at:',\n",
              "   'for eliminating unnecessary weight parameters or by selecting   only important subnetworks within the model. This reduces model size while maintaining   accuracy as high as possible. Finally, distillation trains a smaller model, using the responses   generated by a larger LLM, to reproduce the output of the larger LLM for a specific domain.   This can significantly reduce the amount of training data, compute, and storage resources   needed for the application.  In certain situations, model distillation can also improve the performance of the model itself   in addition to reducing resource requirements. This happens because the smaller model can   combine the knowledge of the larger model with labeled data, which can help it to generalize   better to new data on a limited use case.The process of distillation usually involves training   a large foundational LLM (teacher model) and having it generate responses to certain tasks,   35  Operationalizing Generative AI on Vertex AI using ML OpsSeptember 2024\\x0cand then having the smaller LLM (student model) use a combination of the LLMs knowledge   as well as task specific supervised dataset to learn. The size and complexity of the smaller   LLM can be adjusted to achieve the desired trade-off between performance and resource   requirements. A technique known as step-by-step distillation20 has proven to achieve   great results.  Deployment, packaging, and serving checklist  Following are the important steps to take when deploying a model on Vertex AI.   □ Configure version control: Implement version control practices for LLM deployments.   This allows you to roll back to previous versions if necessary and track changes made to   the model or deployment configuration.   □ Optimize the model: Perform any model optimization (distillation, quantization, pruning,   etc.) before packaging or deploying the model.   □ Containerize the model: Package the trained LLM model into a container.    □ Define target hardware requirements: Ensure the target deployment environment   meets',\n",
              "   'on specific tasks   and contexts within domain-specific datasets. The required examples resemble the one-  shot example structure employed in the construction of a prompt. This effectively extends   the few-shot learning approach for enhanced optimization. This focused tuning enables   the model to encode additional parameters in the model necessary for mimicking desired   behaviors such as improved complex prompt comprehension, adaptation to specific   output formats, correcting errors, and learning new tasks. The SFT tuning approach on   Vertex AI, minimizes computational overhead and time while yielding an updated model   that integrates the newly acquired parameters with the original model’s core parameters.  •  Reinforcement learning with human feedback (RLHF),64 available on Vertex AI for  foundational models like PaLM 2,and open-source models like T5 (s-xxl) and Llama2,   leverages human feedback to train large models to align with human preferences. This   technique is well-suited in complex tasks involving preference modeling and optimizes   LLMs on intricate, sequence-level objectives not easily addressed by traditional   supervised fine-tuning. The process involves first training a reward model using a human   preference dataset, then utilizing it to score the output from the LLM, and finally applying   reinforcement learning to optimize the LLM. This approach is recognized as a key driver of   success in conversational large language models.  •  Distillation step-by-step20 is an advanced distillation technique transferring knowledge  from a significantly larger model (known as teacher model) to a smaller task-specific   model (known as student model), preserving important information while reducing model   size. Step-by-Step Distillation20 surpasses common techniques by requiring significantly   less data. This method, accessible on Vertex AI,65 significantly reduces inference costs and   latencies while minimizing performance impact in the resulting smaller LLM.66  50  Operationalizing Generative AI'],\n",
              "  'response': \"I'm sorry, I could not find any information about the number of layers the Encoder of Transformer is composed of in the provided context. It seems the focus of the text is on operationalizing generative AI, including infrastructure validation, model compression and optimization, and various AI products and services.\",\n",
              "  'reference': 'The encoder is composed of a stack of N = 6 identical layers'}]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "evaluation_dataset = Dataset.from_list(dataset)"
      ],
      "metadata": {
        "id": "JfZP85U-vKNW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2--uUerlqe3W",
        "outputId": "01e725fa-f001-4c6d-b085-a677e51c687d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['user_input', 'retrieved_contexts', 'response', 'reference'],\n",
              "    num_rows: 2\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use the same LLM or different LLM like OpenAI to evaluate"
      ],
      "metadata": {
        "id": "A-NZfSxswtoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "OpenAI_API = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = OpenAI_API\n",
        "open_ai_llm = ChatOpenAI(model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "pbwH9QBZtAa1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 3 metrics which we will be using\n",
        "\n",
        "\n",
        "1. LLMContextRecall: Evaluates how well retrieved contexts align with claims in the reference answer, estimating recall without manual reference context annotations.\n",
        "\n",
        "2. Faithfulness: Assesses whether all claims in the generated answer can be inferred directly from the provided context.\n",
        "\n",
        "3. Factual Correctness: Checks the factual accuracy of the generated response by comparing it with a reference, using claim-based evaluation and natural language inference."
      ],
      "metadata": {
        "id": "DPgcJUXprGvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(open_ai_llm)\n",
        "\n",
        "result = evaluate(\n",
        "    llm=evaluator_llm,\n",
        "    dataset = evaluation_dataset,\n",
        "    metrics=[\n",
        "        LLMContextRecall(llm=evaluator_llm),\n",
        "        FactualCorrectness(llm=evaluator_llm),\n",
        "        Faithfulness(llm=evaluator_llm)\n",
        "    ]\n",
        ")\n",
        "\n",
        "df = result.to_pandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b202d0dacb7842a3bc956b6593ecf8fc",
            "3d365c753ff84dfea8a1263a81c0cae3",
            "5dd557e80eb642d7905889c03fce4b54",
            "6dda76a90efb47edbb2ffa5c4a7b4602",
            "160fadcdb18c49e3916e590fd20ed8b6",
            "adf9802be90f49e29d758eb8e69dffd1",
            "f8b6ee208da6407192e0190b0a6d6dbe",
            "a864ecdfd6854a11bec5599ac16097da",
            "7e290443588d43f68de62bf003108415",
            "c33216b44d7542b6a806d29f4ff58a21",
            "5e0c3622b837482680e64ffda3615739"
          ]
        },
        "id": "I8U8cn43n_Ic",
        "outputId": "a6ea6e51-f910-493f-e7e2-35f166d01dba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b202d0dacb7842a3bc956b6593ecf8fc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.head())"
      ],
      "metadata": {
        "id": "Jz1q8c69td3G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "eac83b4d-cc4f-4ced-b906-4cd664bd9aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          user_input  \\\n",
              "0    What is the score of YOLO on VOC 2012 test set?   \n",
              "1  How many layers does the Encoder of Transforme...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [shown for a variety of detection methods. YOL...   \n",
              "1  [mechanism instead of sequence- aligned recurr...   \n",
              "\n",
              "                                            response  \\\n",
              "0    YOLO scores 57.9% mAP on the VOC 2012 test set.   \n",
              "1  The Encoder of the Transformer is composed of ...   \n",
              "\n",
              "                                           reference  context_recall  \\\n",
              "0   On the VOC 2012 test set, YOLO scores 57.9% mAP.             1.0   \n",
              "1  The encoder is composed of a stack of N = 6 id...             1.0   \n",
              "\n",
              "   factual_correctness  faithfulness  \n",
              "0                  1.0           1.0  \n",
              "1                  1.0           1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68555616-cf8b-4d44-9ad9-0913ee7ca34b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>factual_correctness</th>\n",
              "      <th>faithfulness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is the score of YOLO on VOC 2012 test set?</td>\n",
              "      <td>[shown for a variety of detection methods. YOL...</td>\n",
              "      <td>YOLO scores 57.9% mAP on the VOC 2012 test set.</td>\n",
              "      <td>On the VOC 2012 test set, YOLO scores 57.9% mAP.</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How many layers does the Encoder of Transforme...</td>\n",
              "      <td>[mechanism instead of sequence- aligned recurr...</td>\n",
              "      <td>The Encoder of the Transformer is composed of ...</td>\n",
              "      <td>The encoder is composed of a stack of N = 6 id...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68555616-cf8b-4d44-9ad9-0913ee7ca34b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68555616-cf8b-4d44-9ad9-0913ee7ca34b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68555616-cf8b-4d44-9ad9-0913ee7ca34b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7a17d3e7-81a6-4e9e-9a8c-670bbeaed0a8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7a17d3e7-81a6-4e9e-9a8c-670bbeaed0a8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7a17d3e7-81a6-4e9e-9a8c-670bbeaed0a8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"How many layers does the Encoder of Transformer is composed of?\",\n          \"What is the score of YOLO on VOC 2012 test set?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retrieved_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"The Encoder of the Transformer is composed of a stack of N = 6 identical layers.\",\n          \"YOLO scores 57.9% mAP on the VOC 2012 test set.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"The encoder is composed of a stack of N = 6 identical layers\",\n          \"On the VOC 2012 test set, YOLO scores 57.9% mAP.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"factual_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}