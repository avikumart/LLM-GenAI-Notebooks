{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avikumart/LLM-GenAI-Transformers-Notebooks/blob/main/TMLC_LLM_projects/RAG/Langchain_LCEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LCEL (LangChain Expression Language) simplifies syntax and it's easy to use for simple chains, but it can get confusing once we add complexity."
      ],
      "metadata": {
        "id": "EgLvqhMgJ7nK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SQW6tyx_J1zR",
        "outputId": "fb0c804e-dcb4-4f1a-ee7b-ee54ac2a6fd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/250.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.7/250.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain cohere langchain-community -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_community.llms import Cohere\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "Uvd_gYsQKfyB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_template(\"{question}\")\n",
        "combine_answers_prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"Given the question: {question} and the below two answers:\n",
        "Answer 1:\n",
        "{answer1}\n",
        "\n",
        "Answer 2:\n",
        "{answer2}\n",
        "\n",
        "Combine the viewpoints of two answers and form a coherent combined answer.\n",
        "\"\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "gnisCuMWKFJq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Cohere(cohere_api_key=userdata.get('COHERE_KEY'),temperature=0.2)\n",
        "model2 = Cohere(cohere_api_key=userdata.get('COHERE_KEY'),temperature=0.2)\n",
        "model3 = Cohere(cohere_api_key=userdata.get('COHERE_KEY'),temperature=0.2)\n",
        "\n",
        "chain1 = prompt | model1 | StrOutputParser()\n",
        "chain2 = prompt | model2 | StrOutputParser()\n",
        "chain3 = combine_answers_prompt | model3 | StrOutputParser()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szXcKXHUKkZB",
        "outputId": "cca42dd4-0fe2-40c8-de31-39a1eb9a61e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-acc3ed040b95>:1: LangChainDeprecationWarning: The class `Cohere` was deprecated in LangChain 0.1.14 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import Cohere``.\n",
            "  model1 = Cohere(cohere_api_key=userdata.get('COHERE_KEY'),temperature=0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What's the best way to stay up to date with latest Large Language Model news? Please keep the answer short and concise, limit to 3 bullet points.\""
      ],
      "metadata": {
        "id": "nqo5m1BlKsyY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer1_output = chain1.invoke(question)"
      ],
      "metadata": {
        "id": "nJe5zfEJK0xL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer1_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2xPzYSfLCmv",
        "outputId": "eca27377-76d3-4c30-b6a9-3e83e384f3a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are three ways to stay informed about the latest news and updates regarding Large Language Models: \n",
            "\n",
            "1. Follow Social Media Accounts: Many researchers, developers, and organizations involved in LLM research and innovation regularly post updates on social media platforms like Twitter. By following key accounts and influencers, you can get a stream of the latest news, articles, and insights about LLMs. \n",
            "\n",
            "2. Subscribe to Research Blogs: Several academic institutions and technology companies working in the field of AI and LLMs maintain research blogs. These blogs provide detailed updates on their work, advancements, and breakthroughs. Subscribing to these blogs ensures you get emailed updates as soon threads are published. \n",
            "\n",
            "3. AI and ML Online Publications: Subscribe to online publications websites that specialize in AI, Machine Learning, and NLP. These websites provide in-depth articles and analyses on various aspects of LLMs, including news about advancements, interviews with experts, and discussions of emerging trends. \n",
            "\n",
            "By combining these three methods, you can effectively stay informed and abreast of the most recent Large Language Model news and advancements. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer2_output = chain2.invoke(question)"
      ],
      "metadata": {
        "id": "sPOp2XVGK1fA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer2_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn1Rk6yLLDPM",
        "outputId": "788f9a16-0e65-4f49-ccde-d3ea25440507"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are three ways to stay informed about the latest news and updates regarding Large Language Models: \n",
            "\n",
            "1. Follow Social Media Accounts: Many researchers, developers, and organizations involved in LLM research and innovation regularly share updates on social media platforms like Twitter. By following key accounts and influencers, you can stay informed about the latest advancements, publications, and events in the field. \n",
            "\n",
            "2. Subscribe to Newsletter: Subscribe to newsletters offered by AI organizations, research labs, and industry publications. These newsletters provide curated updates about LLM innovations, research breakthroughs, and market developments directly to your inbox. \n",
            "\n",
            "3. Participate in Online Communities: Join online forums, message boards, and specialized platforms where experts and enthusiasts discuss AI, LLMs, and related topics. Participating in these communities allows you to connect with knowledgeable individuals, gain insights, and stay updated on the latest news and trends in the LLM world. \n",
            "\n",
            "These options offer a balanced approach to staying informed about LLM news and developments, allowing you to tailor your information sources to your preferences. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_answer_output = chain3.invoke({\"question\": question, \"answer1\": answer1_output, \"answer2\": answer2_output})"
      ],
      "metadata": {
        "id": "Io0l0Dm9K2lg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_answer_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Otu5bsqLJP4",
        "outputId": "0c24507f-a75f-48ff-f500-8e77e7f5f300"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are three ways to stay informed about the latest news and updates regarding Large Language Models: \n",
            "\n",
            "1. Follow Social Media Accounts: Many researchers, developers, and organizations involved in LLM research and innovation regularly share updates on social media platforms like Twitter. By following key accounts and influencers, you can stay informed about the latest advancements, publications, and events in the field. \n",
            "\n",
            "2. Subscribe to Newsletter: Subscribe to newsletters offered by AI organizations, research labs, and industry publications. These newsletters provide curated updates about LLM innovations, research breakthroughs, and market developments directly to your inbox. \n",
            "\n",
            "3. Participate in Online Communities: Join online forums, message boards, and specialized platforms where experts and enthusiasts discuss AI, LLMs, and related topics. Participating in these communities allows you to connect with knowledgeable individuals, gain insights, and stay updated on the latest news and trends in the LLM world. \n",
            "\n",
            "By combining these three methods, you can effectively stay informed and abreast of the most recent Large Language Model news and advancements. \n",
            "Let me know if you'd like me to elaborate on any of the aforementioned methods to stay up-to-date with LLM news. I can provide some relevant Twitter handles or online communities if you'd like to get started\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_chain = {\n",
        "    \"question\": RunnablePassthrough(),\n",
        "    \"answer1\": chain1,\n",
        "    \"answer2\": chain2,\n",
        "} | chain3\n",
        "\n",
        "combined_answer = combined_chain.invoke(question)"
      ],
      "metadata": {
        "id": "AgRYc8yKK9o6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfqEHoZXLUjB",
        "outputId": "85da2b2b-dbbe-4b29-ba38-906705c46b45"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Here are three ways to stay informed about the latest news and updates regarding Large Language Models: \n",
            "\n",
            "1. Follow Social Media Accounts: Many researchers, developers, and organizations involved in LLM research and innovation regularly share updates on social media platforms like Twitter. By following key accounts and influencers, you can stay informed about the latest breakthroughs, announcements, and discussions regarding LLMs. \n",
            "\n",
            "2. Subscribe to Research Blogs: Several academic institutions, tech companies, and individuals maintain blogs focused on language technology and AI. Subscribing to these blogs will provide you with monthly digest emails of their latest posts, ensuring you don't miss any important updates. \n",
            "\n",
            "3. Combine Journalism Platforms with Attendance of Conferences and Events: Following journalism platforms like TechCrunch, The Verge, and Wired, along with attending conferences and virtual events, will provide a balanced perspective on the LLM industry. Journalism platforms offer in-depth analysis and timely updates, while event participation provides direct interaction with experts and the opportunity to form and exchange opinions with like-minded individuals. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_chain = (\n",
        "    {\"question\": RunnablePassthrough()}\n",
        "    | RunnablePassthrough.assign(answer1=chain1)\n",
        "    | RunnablePassthrough.assign(answer2=chain2)\n",
        "    | chain3\n",
        ")"
      ],
      "metadata": {
        "id": "YbEWm2qcK-46"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(combined_chain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X3jKd0KLWIo",
        "outputId": "155b5a30-5a44-4143-c3a0-dc979592a2af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first={\n",
            "  question: RunnablePassthrough()\n",
            "} middle=[RunnableAssign(mapper={\n",
            "  answer1: ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
            "           | Cohere(client=<cohere.client.Client object at 0x7eabd63bac20>, async_client=<cohere.client.AsyncClient object at 0x7eabd545a4d0>, temperature=0.2, cohere_api_key=SecretStr('**********'))\n",
            "           | StrOutputParser()\n",
            "}), RunnableAssign(mapper={\n",
            "  answer2: ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
            "           | Cohere(client=<cohere.client.Client object at 0x7eabd54581f0>, async_client=<cohere.client.AsyncClient object at 0x7eabd5459060>, temperature=0.2, cohere_api_key=SecretStr('**********'))\n",
            "           | StrOutputParser()\n",
            "}), ChatPromptTemplate(input_variables=['answer1', 'answer2', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['answer1', 'answer2', 'question'], input_types={}, partial_variables={}, template='Given the question: {question} and the below two answers:\\nAnswer 1:\\n{answer1}\\n\\nAnswer 2:\\n{answer2}\\n\\nCombine the viewpoints of two answers and form a coherent combined answer.\\n'), additional_kwargs={})]), Cohere(client=<cohere.client.Client object at 0x7eabd60115d0>, async_client=<cohere.client.AsyncClient object at 0x7eabd55cf370>, temperature=0.2, cohere_api_key=SecretStr('**********'))] last=StrOutputParser()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA9Wp2FNIPwJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}